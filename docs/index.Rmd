---
title: "Programming in R"
author: "Mattia Toninelli"
date: "2/6/2023"
output: 
  html_document:
    toc: true
    theme: united
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = 'center', warning = FALSE, message = FALSE)
```

# Introduction
The course will give an overview of R as an instrument for programming and efficient scripting. It will start with the basics and then develop into an exploration of ways to improve code performance and structure. All of the materials from the lectures can be found [here](https://www.ceredalab.com/SPC/index.html), with relevant information related to the authors as well.

## Day 1 - Programming history and R
#### A brief history of programming
The existence of programming languages is strongly related to how computers work and how they have been developed during their history. Architecture is the base on which languages are built.
Computers are built and organized in what is known as [Von Neumann architecture](), in this sense we have a CPU which is the main component involved in calculations. This is in turn composed of an arithmetic/logic unit a control unit and registers (MAR and MDR) on which data is kept pre-operation after it has been sourced from the memory unit (RAM). In between the memory and the CPU lies the _cache_ (L1, L2) which hosts information that can be retrieved in a faster way than from memory. 

![Von Neumann architecture for computers.](http://computerscience.gcse.guru/wp-content/uploads/2016/04/Von-Neumann-Architecture-Diagram.jpg#center)

So, **what is a program**? Memory and CPUs can only understand numbers, specifically bits of information, 0s and 1s and are only able to perform very simple operations, meanwhile higher-level operations are achieved by using _algorithms_ which compound simpler operations iteratively. The first step towards translating bit instructions into alphanumeric meaningful words was the _Assembly_ language. Assembly worked by using procedure (or routines) calling, we can think of routines as blocks of codes to execute in larger scripts.
Part of this process was also involving memory management procedures, with pre-allocated memory (_static_) to store all global variables in a last-in first-out way and this set the initial mechanism for running programs, other parts of memory include the _stack_, which is the part related to local variables and programs and then a part of the memory reserved to code (or text in general), called _text_ and finally a part known as _heap_ which is, in line with its name, very dynamic and changes in relation to changes in need at runtime, this part of memory is the one which can be controlled by the programmer (in **compiled languages** usually) or by programs themselves (in **interpreted languages** usually) as well, here programs can be restricted in terms of usage. This was mainly engineered to avoid memory conflicts between running programs, so it became crucial to control the system and _assign memory_ to running processes in a consistent and stable manner. This task was tackled by developing **operating systems** (OS).

![Memory allocation strategy, dependant on OS configuration.](https://study.com/cimages/multimages/16/1f3a8c28-4082-4714-b6fa-6bb81804095b_screen_shot_2017-12-01_at_1.31.10_pm.png#center)

The OS is instrumental in establishing order in resource and memory management across programs running on the same machine from the same or different users. Another step towards this is to share a machine's CPU for _time-sharing_ across processes running at the same time, all this is programmed within the OS, which is tasked with managing resources.
In the 80s and 90s, with the advent of networking, distributed systems were created in which programs could be run on different machines by taking advantage of web-based protocols (like `ssh`).
After the initial rise of OSs, forms of _mass storage_ were needed in order to have a memory support built into the computer itself, this started with CDs and then hard-drives, which nowadays have been substituted in large by faster technologies such as SSDs and cloud-based solutions for storing data.

All along, **expressive languages** were also developed which were needed to get a more abstract sense of operations and allow for more comprehensible syntax for more complex operations like cycling and conditional execution, introduced a grammar based on data structures, types and objects. Ultimately this allows the programmer to focus on the actual program itself more than its implementation.

So summed up, the advantages of languages include:
+ Loops
+ Conditional execution
+ Code blocks
+ Operators
+ Data structures (specifications)
+ Function definition

Objects in memory are always stored sequentially and specifically, matrices and tabular data in Fortran systems (and the majority of other languages) are stored column-wise, so trying to index a matrix like `A[57] == A[7,6]` means that the 57th element of a matrix in memory is equal to the 7th element on the 6th column in the R object (which is 1-indexed). 

```{r}
index <- c(1,2,3,NA)
random <- runif(100)

# Try to use NA as index (should return an NA in that index position)
# This highlights a fault in R's permessibility, here it does not alert the use for the sake of giving on answer
random[index]
```
```{r, echo=FALSE}
rm(index)
rm(random)
```

In the above case we can check that R, an interpreted language, masks the user with the ability to manage an eventual error for the sake of code usability, the user then misses a chance to catch an eventual mistake in the code, maybe while generating the index above, an `NA` was not wanted, but R does not signal its presence while subsetting `random`. This mechanisms of silent coercion are widespread in R and the user should always be aware of when they can happen since they can represent a scenarion of bug generation and data loss.

#### Data structures and Object-oriented programming
##### Vectors
In R, we can have beginner data structures (i.e. vectors, matrices and dataframes) and more advanced structures (i.e. S3, S4 and RC). Simpler data structures can further be subset into _homogeneous_ or _heterogeneous_ data structures. 
**Atomic vectors** and **lists** differ due to the type of data they can contain, one is formed by elements of the same type, while lists are heterogeneous.
Operations based on vectors in R are element-wise, meaning that vector-vector operations are done element by element (_with the shorter vector being recycled_) while scalar-vector operations broadcast the operation to each single element of a vector. 

```{r}
# In R, we can function the structure function to check for the structure of the data 
# This coerces the integer
v <- c('q', 1, 'a', 2)

# Look at structure
str(v)

# Additionally, we can set attributes to values for variables
'a_name' <- attr(v, 'my_attribute')
```

Another functionally important aspect is to remind ourselves of the existance of approximations in calculations, in this case if we type the following:

```{r}
x <- (sqrt(2))**2
x - 2 # Should return 0 but it does not
```

This happens due to the approximation that the machine provides, and this is why we need to pay a great deal of attention when using conditionals with `== 0`. This is partially rescued by the use of a function like the one `dplyr::near()`.

Hand in hand with this concept is the one of _coercion_, by which object get converted from their original class to other classes. This in R happens either _implicitly_ or _explicitly_.

Creating sequences of numbers in R is fairly straightforward with many different functions available and we can generate both random sequences, repeated sequences or specific distributions (normal, poisson or others) by specifying the needed parameters.

```{r}
# Genrate 100 random doubles from 0 to 10
dbls <- runif(100, min=0, max=10)

# Sample 100 values from a standard normal distribution
dbls <- rnorm(100, mean=0, sd=1)

# Generate 100 random outcomes from a coin flip using a Bernoullu trial
dbls <- rbinom(100, size=1, prob=0.5)

# Sample 100 values from a negative binomial distribution with size and prob values
dbls <- rnbinom(100, size=1, prob=0.75)
```

##### Matrices
Two-dimensional data in R is represented by the `Matrix` class and can be constructed using the following code:

```{r}
# Create a matrix
mat <- matrix(runif(60), ncol=3)

# Assign different dimensions
dim(mat) <- c(3,20) # 3 rows 20 cols

# Isolate the upper triangle of the matrix in index form
idxs <- upper.tri(mat, diag = FALSE)
```

Expanding on operations, matrix multiplication in R is achieved with the operator `%*%` as such:

```{r}
# Create matrices
x <- matrix(runif(10), ncol = 5)
y <- matrix(runif(5), nrow = 5)

# rows by col matrix multiplication
matmul <- x %*% y

matmul
```

##### Lists
Lists are probably the most programmatically complex objects in R, they contain different types and have different lenght with or without named elements. They represent the very first interface to **Object-oriented programming**.

##### Object-Oriented (OO) data type
Here we are moving from basic structures to objects which can contain data and are associated to methods which are able to operate on the data itself. It is a mode of operation borrowed from C++. A **class** contains _methods_ and _attributes_, the whole creates one of the main instances of OOP. Class elements can inherit from other objects which share the same parent. The main thing to understand is that R, by default, treats everything as an object. Everything, literally _everything_, has attributes and maybe even methods, from basic vectors to complex classes.

```{r}
vec <- c(1,2,3)

# Illustrate classes
class(vec)

# Coerce vector to another class as well, so that methods designed with that class im mind, can be applied to this object as well
class(vec) <- append(class(vec), 'SPC')

# Now vec will have both classes listed
class(vec)
```

The way we can think of classes in R is that they are description of things in a specific methods system and are defined by the `setClass()` function in the `methods` package (in S4). An object is an instance of a class, these can be created using the `new()` function. Finally a method is a function operating _only_ on a certain class of objects, in this sense a `generic` function is one which _dispatches methods_, this means that they do not perform any computation, they just figure out the class of the data and matches it with an appropriate method. Generics can of course be created and a developer might create associated methods. For S3 system objects there is also a `methods` function which lets the user explore methods implemented for the function.

By default, R has three object oriented systems, one is **S3**, it implements classes without defining them strictly but instead defines functions and methods separately across classes. The main functional aspect related to S3 is the ability to _overload functions_, this means that one function can be swiftly applied to different class elements and behave correctly. S3 functions are by design split into `generics` and `methods` and are defined in the code by using `generic.class`. 

```{r}
# Example of an S3 defined (generic) function is print
print

# Call methods function on a generic function, in this case mean
methods('mean')
```

S3 pre-dates S4 both in terms of time and functionality, S3 methods system is less rigorous and informal, this called for a more formalized approach tackled by S4.

We can explicitly access the code behind an S3 method definition by doing the following:

```{r}
head(getS3method('mean', 'default'))
tail(getS3method('mean', 'default'))
```

This shows the inner functionality of the S3 method behind `mean`. Interestingly, the `.Internal` at the very end is used to fetch C code which adds functionality to the method.

**S4** formally defines classes, while **RC** (reference class) binds also the methods and has a very similar approach to classes in C++. In the below code we can see how, differently from S3, an S4 method does not have ellipses in the `standardGeneric` function since it takes on standardized input invariably.

```{r}
# Print out an S4 function
show
```

Classes in R can be defined using the `structure()` function by doing something like `foo <- structure(list(), class = 'my_class')` and then from there we can start building methods for the class by specifying functions operating on the structure.
A hierarchy of sorts can be visualized as follows, first we have a class, then in **S3** methods are below functions, which in turn are separated from the class specification (like class and attributes in python).
**RC** style objects are the ones closer to the object-oriented programming way of C++, here methods and attributes are packaged within the class.

Now, as an example, we will define a simple S4 class for describing a bank account.

```{r}
# Define the S4 class with slots
setClass('bank_account',
         representation('user' = 'character',
                        'balance' = 'numeric'))
```

In this way, we are able to access the slots of the class with the `@` operator, which is specifically reserved to access S4 class slots. Let's now create a new method which allows the updating of the class object after a money deposit.

```{r}
# A method to update a class 
setGeneric('sum')

setMethod('sum', # specify a generic function 
          'bank_account', # signature
          function(x, deposit) {
            x@balance <- x@balance + deposit
          })

# If we call methods on print we should see the new method as well
showMethods('sum')
```

Now let's try the new class in action and apply the defined method to an object!

```{r}
# Create a new bank account with user and balance
ba <- new('bank_account', user = 'Mattia', balance = 100)

# Add a deposit (if this was a Ref Class then updates could happen inplace)
ba@balance <- sum(ba, 200)

# Print out new value
ba@balance
```

##### I/O - Working with files
In R, we are able to work with files by interacting with the host OS and filesystem. The function `read.table()` is a built-in which helps the user to read files. In addition, by using `write.table()` we can save a table object with a specified name and separator.
R also provides a specific function to `dump()` a whole environment image into a variable which can be save, although this is highly inefficient.

Additionally the `tidyverse`, through `readr`, provides a faster interface to achieve the same I/O management. 

An interesting binary file format to save objects in R but which can be read by any other programming languages is the one provided by the `feather` library.

