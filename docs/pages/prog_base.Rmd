---
output: 
  html_document:
    toc: true
    toc_float: true
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = 'center', warning = FALSE, message = FALSE)
```

## A brief history of programming
### The Architecture
The existence of programming languages is strongly related to how computers work and how they have been developed during their history.
Architecture is the base on which languages are built.
Computers are built and organized in what is known as [Von Neumann architecture](), in this sense we have a CPU which is the main component involved in calculations.
This is in turn composed of an arithmetic/logic unit a control unit and registers (MAR and MDR) on which data is kept pre-operation after it has been sourced from the memory unit (RAM).
In between the memory and the CPU lies the *cache* (L1, L2) which hosts information that can be retrieved in a faster way than from memory.

::: {align="center"}
<img src="http://computerscience.gcse.guru/wp-content/uploads/2016/04/Von-Neumann-Architecture-Diagram.jpg"/>
:::

### Programs
So, **what is a program**?
Memory and CPUs can only understand numbers, specifically bits of information, 0s and 1s and are only able to perform very simple operations, meanwhile higher-level operations are achieved by using *algorithms* which compound simpler operations iteratively.
The first step towards translating bit instructions into alphanumeric meaningful words was the *Assembly* language.
Assembly worked by using procedure (or routines) calling, we can think of routines as blocks of codes to execute in larger scripts.
Part of this process was also involving memory management procedures, with pre-allocated memory (*static*) to store all global variables in a last-in first-out way and this set the initial mechanism for running programs, other parts of memory include the *stack*, which is the part related to local variables and programs and then a part of the memory reserved to code (or text in general), called *text* and finally a part known as *heap* which is, in line with its name, very dynamic and changes in relation to changes in need at runtime, this part of memory is the one which can be controlled by the programmer (in **compiled languages** usually) or by programs themselves (in **interpreted languages** usually) as well, here programs can be restricted in terms of usage.
This was mainly engineered to avoid memory conflicts between running programs, so it became crucial to control the system and *assign memory* to running processes in a consistent and stable manner.
This task was tackled by developing **operating systems** (OS).

::: {align="center"}
<img src="https://i.stack.imgur.com/HOY4C.png"/>
:::

The OS is instrumental in establishing order in resource and memory management across programs running on the same machine from the same or different users.
Another step towards this is to share a machine's CPU for *time-sharing* across processes running at the same time, all this is programmed within the OS, which is tasked with managing resources.
In the 80s and 90s, with the advent of networking, distributed systems were created in which programs could be run on different machines by taking advantage of web-based protocols (like `ssh`).
After the initial rise of OSs, forms of *mass storage* were needed in order to have a memory support built into the computer itself, this started with CDs and then hard-drives, which nowadays have been substituted in large by faster technologies such as SSDs and cloud-based solutions for storing data.

### Memory
All along, **expressive languages** were also developed which were needed to get a more abstract sense of operations and allow for more comprehensible syntax for more complex operations like cycling and conditional execution, introduced a grammar based on data structures, types and objects.
Ultimately this allows the programmer to focus on the actual program itself more than its implementation.

So summed up, the advantages of languages include: 
+ Loops 
+ Conditional execution 
+ Code blocks 
+ Operators 
+ Data structures (specifications) 
+ Function definition

Objects in memory are always stored sequentially and specifically, matrices and tabular data in Fortran systems (and the majority of other languages) are stored column-wise, so trying to index a matrix like `A[57] == A[7,6]` means that the 57th element of a matrix in memory is equal to the 7th element on the 6th column in the R object (which is 1-indexed).

```{r}
index <- c(1,2,3,NA)
random <- runif(100)

# Try to use NA as index (should return an NA in that index position)
# This highlights a fault in R's permessibility, here it does not alert the use for the sake of giving on answer
random[index]
```

```{r, echo=FALSE}
rm(index)
rm(random)
```

In the above case we can check that R, an interpreted language, masks the user with the ability to manage an eventual error for the sake of code usability, the user then misses a chance to catch an eventual mistake in the code, maybe while generating the index above, an `NA` was not wanted, but R does not signal its presence while subsetting `random`.
This mechanisms of silent coercion are widespread in R and the user should always be aware of when they can happen since they can represent a scenarion of bug generation and data loss.

## Parallel Programming
### The Basics
When programmming something using *interpreted* (vs *compiled*, i.e. C++) languages like R, primarily thought as interactive, it is important to stop and understand how functions work in terms of operations and I/O instead of just getting into the thick of it and start using up resources.
This is primarily concerning memory requirements when working on a shared platform as well as actual CPU resources.
*Dynamic vs Static* libraries.
To provide an example of the performance that can be obtained by programming in compiled languages, let's imagine a scenario by which we have a series of DNA sequences and we want to get all of the 3-base *kmers* which are present in each sequence.
One relatively straightforward way would be to first generate an empty sequence by kmer matrix and then, after having determined all the possible kmers in the sequences (a dictionary), start counting and indexing any occurrence into the appropriate position.
Although this works well with short sequences, the advantages which come from the conjunction of [binary encoding](https://www.sciencedirect.com/science/article/pii/S0888754316300854) and the use of C++ which allows the direct accession of space on memory grant amazing advantages.
Brush-up on the definition of a [core and a thread](https://www.temok.com/blog/cores-vs-threads/)?
But another way to brute-force achieving speed is to use more cores and threads!
In order to take advantage of the presence of multiple cores, we can run different processes by sizing the data into smaller chunks (easier, less efficient) or make a program **multithread**, this is the best way possible but also the most expensive in terms of implementation time.
By multithreading we split up the instructions of a program into spawning different children processes.
Parallelization comes with a few challenges, including timing and managing execution I/O across child processes.
There are cases in which data needs to be modified by different processes thus we need to ensure that we are not compromising the outputs and/or mixing information from different processes.

### Graphics Processing Units
GPUs work very well for linear algebra calculations, especially linear algebra concepts and operations.
GPUs are very "dumb" compare to CPUs in terms of the kinds of operations and the complexity of operations that they can do, but what they were initially designed to do (rendering graphics, therefore large matrices) proved also very powerful when applied to operations involving matrices.
These cards have a very different architecture as opposed to the Von Neumann one seen previosuly, they contain many more threads doing the same operation on the same data, which needs, *crucially*, to be copied on the CPU itself, which will also store the final object after computation which will need to be re-imported into memory.

### OOP History and Paradigms
Programming has taken up many shapes during its history, we can have *unstructured programming* which then evolved into *procedural programming* by definig re-usable functions which otherwise need repetition.
The next step in complexity is represented by *modular programming*, by which modules of code are utilized and interconnect to provide functionality across different files.
The final step is represented by the *object-oriented* paradigm, by which objects are identified by a set of methods coupled with the needed data, which in turn is strictly related to the methods.
Let's imagine that we want to create a new genomics object in which to store genomic data, this object can be as general as possible, with specific properties and methods.
The former might contain IDs from different classification systems, the annotation and the organism of origin while the latter might instead contain actions which can be performed on the object such as alignment, changes in bases or calculation of GC content etc... In contrast to procedural programming, the program flow in OOP is said to be "message oriented", by this meaning that the user talks to objects by sending messages through methods.
The interesting part of OOP is that classes are not just created to be on their own, instead **relationships** between objects is desirable as a distinguished feature, for example an object representing a transcript can *be part of* an object representing a chromosome and can *inherit* actions which can be performed on other elements from the chromosome object, for example determining start and end locations, which could also be use for a genomic sequence, which in turn can be another class under the chromosome one.
Methods defined in a parent class are re-implemented in the daughter classes, in this way in the code we can always call the same method and the correct implementation will always be selected since it is specified in the appropriate class, this is called *polymorphism*.
