[
  {
    "objectID": "pages/index.html",
    "href": "pages/index.html",
    "title": "Programming in R",
    "section": "",
    "text": "Introduction\nThe course will give an overview of R as an instrument for programming and efficient scripting, with a twist on using it to analyze biologically relevant data. It will start with the basics and then develop into an exploration of ways to improve code performance and structure. All of the materials from the lectures can be found here, with relevant information related to the authors as well.\nThe course, which lasted one week, has been thaught by U. Pozzoli, F. Iannelli and M. Cereda at IEO in Milan."
  },
  {
    "objectID": "pages/r_genomics.html",
    "href": "pages/r_genomics.html",
    "title": "R Programming Course",
    "section": "",
    "text": "Bioconductor was initially created with the idea in mind to generate an open-source place based on the R programming language on which to store packages and projects for the analysis of genomics data. Given the nature of Bioconductor, it encourages contributions and learning through massive documentation bundled with every package. Bioconductor packages are focused on deploying many different functionalities spanning from data visualization for genomics to differential expression analysis, all of this coupled with the added learning bonuses coming from the release of vignettes. Some of the most used packages in Bioconductor used for genome arithmetics include IRanges and GenomicRanges. Since the genome is a linear one-dimensional coordinate system, we can perform different kinds of operations related for instance to overlaps. For example in the former package, we can define a set of three ranges by using the following code:\nMost of the provided operations above are recalling ones already existant in C-based packages like bedtools.\nAnother way of visualizing genomics elements is through strings, after all DNA sequences are just sets of letters. A useful suite of commands is the one provided by the package Biostrings. This provides a multitude of functions that provide the ability to generate for instance reverse complements of the original sequence. This for instance is useful since in FASTA format files, the genes are always present with their forward strand and therefore we might need the reverse complement if the gene is antisense. This capability is achieved with the DNAString class and its associated methods.\nBioconductor already contains many packages which gather genetic information for many model organisms, one of them is BSgenome while others live within the .org family of packages."
  },
  {
    "objectID": "pages/r_genomics.html#genomics-data-visualization",
    "href": "pages/r_genomics.html#genomics-data-visualization",
    "title": "R Programming Course",
    "section": "Genomics data visualization",
    "text": "Genomics data visualization\n\nPlotting basics in R\nOne of the main uses for data visualization is to perform exploratory data analysis (EDA). The basic library in R for creating customisable and clear visualization is ggplot. This package is inserted within a rich environment of data manipulation tools and visualization called tidyverse. The fundamental idea related to data visualization is to communicate a story using the data at our disposal. Relationships hidden in data stand out with proper visualization techniques and ideas. That’s the thing, it’s not only about dull application but also about creativity and storytelling. This phenomenon is especially relevant while working with huge datasets where relationships can exist in many directions and between many variables at once, and where each variable might be represented by thousands or millions of observations. This is exactly the case for genomic data sets describing relevant biological information.\n\n# Generative art with R example with folded code?\n\nIn its basic functionality, ggplot exploits what is known as “the grammar of graphics”, gg. This consists of a layered mechanism of instructions. We start by defining which variables of interest are to map to which plot characteristic between color, fill, shape and size. This happens by using the aes() function within either a direct ggplot call or within geoms. geoms are used to define what kind of plot to use, and this of course is going to depend on which variable relationships we want to highlight. One of the paradigms of plotting data is the variable-chart type relationship. By this we mean the right coupling between chart type and appropriate variable nature to plot (i.e. continous vs discrete).\nThese are the main relationships to take into account when plotting: + One variabel, continous: histogram, density curve (distributions) + One variable, discrete: bar chart, pie chart (proportions) + Two variables, continuous: scatter plot, 2D density kernels (distributions) + Two variables, discrete: confusion matrices, heatmaps (common occurence) + Two variables, one continuous and one categorical: box plot, violin plot (distributions)\nThis, in practice, for a minimal plot between two continuous variables:\n\nlibrary(ggplot2)\n\ndata <- mtcars\n\n# Plot the relationship between a car displacement and mpg, colored by number of cylinders and give a dot size related to the car's weight\n# From this it appears clear how heavier cars have bigger engines, consume more and have more cylinders.\nggplot(data, aes(x = disp, y = mpg, col = as.factor(cyl))) +\n    geom_smooth(color='black') +\n    geom_point(aes(size = wt), alpha=0.8) +\n    theme_minimal() \n\n\n\n\n\n\n\n\nObviously there exist many extensions which add functionality to the basic package. For example we can plot maps with map_data (or leaflet widgets) or alluvial diagrams (a subset of Sankey diagrams) with ggalluvial. Another useful package when studying clonal evolution (in tumors particularly) is fishplot, which creates publication-ready plots.\n\n\nPlotting heatmaps\nHeatmaps are visual representations of tabular data. One of the most used packages to create heatmaps is pheatmap and its more personalizable counterpart complexHeatmap.\n\nlibrary(ComplexHeatmap)\n\n\n\nUpSet plots\nSet operations can be depicted with the use of Venn diagrams, but what it we need more complex operations spanning more than four sets? (the Venn of which can already be quite confusing) In this use case we can check out the package UpSetR which will produce plots line the one below. In this case we can see how it is useful in exploring multiple set relationships all at once. UpSet plots can also be thought of as extensions of heatmaps, by coloring sets by intersection size and annotations.\n\n\nOncoprint\nOncoPrint became very famous in CNV studies in cancer, these plot are dedicated to checking out gene alterations as matrices where samples are columns and genes are rows.\n\n\ncBioPortal for Cancer Genomics\nThis is a website hosted by MSKCC where cancer cases can be collected in repositories which can be queried as normal SQL databases to extract relevant metadata and genomic data coming from a great amount of studies. These sources can be accessed via the web and can be downloaded to re-create plots.\n\n\nGenome-level Heatmaps\nHere we are producing genome-level heatmaps divided by chromosome and/or genes. These can be further decorated and transformed into visually pleasing plots by using additional libraries like circlize, which is the R implementation of Circos plots.\n\nlibrary(circlize)"
  },
  {
    "objectID": "pages/r_paradigms.html",
    "href": "pages/r_paradigms.html",
    "title": "R Programming Course",
    "section": "",
    "text": "In R, we can have beginner data structures (i.e. vectors, matrices and dataframes) and more advanced structures (i.e. S3, S4 and RC). Simpler data structures can further be subset into homogeneous or heterogeneous data structures. Atomic vectors and lists differ due to the type of data they can contain, one is formed by elements of the same type, while lists are heterogeneous. Operations based on vectors in R are element-wise, meaning that vector-vector operations are done element by element (with the shorter vector being recycled) while scalar-vector operations broadcast the operation to each single element of a vector.\n\n# In R, we can function the structure function to check for the structure of the data \n# This coerces the integer\nv <- c('q', 1, 'a', 2)\n\n# Look at structure\nstr(v)\n\n chr [1:4] \"q\" \"1\" \"a\" \"2\"\n\n# Additionally, we can set attributes to values for variables\n'a_name' <- attr(v, 'my_attribute')\n\nAnother functionally important aspect is to remind ourselves of the existance of approximations in calculations, in this case if we type the following:\n\nx <- (sqrt(2))**2\nx - 2 # Should return 0 but it does not\n\n[1] 4.440892e-16\n\n\nThis happens due to the approximation that the machine provides, and this is why we need to pay a great deal of attention when using conditionals with == 0. This is partially rescued by the use of a function like the one dplyr::near().\nHand in hand with this concept is the one of coercion, by which object get converted from their original class to other classes. This in R happens either implicitly or explicitly.\nCreating sequences of numbers in R is fairly straightforward with many different functions available and we can generate both random sequences, repeated sequences or specific distributions (normal, poisson or others) by specifying the needed parameters.\n\n# Genrate 100 random doubles from 0 to 10\ndbls <- runif(100, min=0, max=10)\n\n# Sample 100 values from a standard normal distribution\ndbls <- rnorm(100, mean=0, sd=1)\n\n# Generate 100 random outcomes from a coin flip using a Bernoullu trial\ndbls <- rbinom(100, size=1, prob=0.5)\n\n# Sample 100 values from a negative binomial distribution with size and prob values\ndbls <- rnbinom(100, size=1, prob=0.75)\n\n\n\n\nTwo-dimensional data in R is represented by the Matrix class and can be constructed using the following code:\n\n# Create a matrix\nmat <- matrix(runif(60), ncol=3)\n\n# Assign different dimensions\ndim(mat) <- c(3,20) # 3 rows 20 cols\n\n# Isolate the upper triangle of the matrix in index form\nidxs <- upper.tri(mat, diag = FALSE)\n\nExpanding on operations, matrix multiplication in R is achieved with the operator %*% as such:\n\n# Create matrices\nx <- matrix(runif(10), ncol = 5)\ny <- matrix(runif(5), nrow = 5)\n\n# rows by col matrix multiplication\nmatmul <- x %*% y\n\nmatmul\n\n          [,1]\n[1,] 1.2873007\n[2,] 0.4896845\n\n\n\n\n\nLists are probably the most programmatically complex objects in R, they contain different types and have different lenght with or without named elements. They represent the very first interface to Object-oriented programming."
  },
  {
    "objectID": "pages/r_paradigms.html#object-oriented-oo-data-type",
    "href": "pages/r_paradigms.html#object-oriented-oo-data-type",
    "title": "R Programming Course",
    "section": "Object-Oriented (OO) data type",
    "text": "Object-Oriented (OO) data type\n\nClasses in general\nHere we are moving from basic structures to objects which can contain data and are associated to methods which are able to operate on the data itself. It is a mode of operation borrowed from C++. A class contains methods and attributes, the whole creates one of the main instances of OOP. Class elements can inherit from other objects which share the same parent. The main thing to understand is that R, by default, treats everything as an object. Everything, literally everything, has attributes and maybe even methods, from basic vectors to complex classes.\n\nvec <- c(1,2,3)\n\n# Illustrate classes\nclass(vec)\n\n[1] \"numeric\"\n\n# Coerce vector to another class as well, so that methods designed with that class im mind, can be applied to this object as well\nclass(vec) <- append(class(vec), 'SPC')\n\n# Now vec will have both classes listed\nclass(vec)\n\n[1] \"numeric\" \"SPC\"    \n\n\nThe way we can think of classes in R is that they are description of things in a specific methods system and are defined by the setClass() function in the methods package (in S4). An object is an instance of a class, these can be created using the new() function. Finally a method is a function operating only on a certain class of objects, in this sense a generic function is one which dispatches methods, this means that they do not perform any computation, they just figure out the class of the data and matches it with an appropriate method. Generics can of course be created and a developer might create associated methods. For S3 system objects there is also a methods function which lets the user explore methods implemented for the function.\n\n\nClass systems\nBy default, R has three object oriented systems, one is S3, it implements classes without defining them strictly but instead defines functions and methods separately across classes. The main functional aspect related to S3 is the ability to overload functions, this means that one function can be swiftly applied to different class elements and behave correctly. S3 functions are by design split into generics and methods and are defined in the code by using generic.class.\n\n# Example of an S3 defined (generic) function is print\nprint\n\nfunction (x, ...) \nUseMethod(\"print\")\n<bytecode: 0x7ff9b2712078>\n<environment: namespace:base>\n\n# Call methods function on a generic function, in this case mean\nmethods('mean')\n\n[1] mean.Date     mean.default  mean.difftime mean.POSIXct  mean.POSIXlt \n[6] mean.quosure*\nsee '?methods' for accessing help and source code\n\n\nS3 pre-dates S4 both in terms of time and functionality, S3 methods system is less rigorous and informal, this called for a more formalized approach tackled by S4.\nWe can explicitly access the code behind an S3 method definition by doing the following:\n\nhead(getS3method('mean', 'default'))\n\n                                                                     \n1 function (x, trim = 0, na.rm = FALSE, ...)                         \n2 {                                                                  \n3     if (!is.numeric(x) && !is.complex(x) && !is.logical(x)) {      \n4         warning(\"argument is not numeric or logical: returning NA\")\n5         return(NA_real_)                                           \n6     }                                                              \n\ntail(getS3method('mean', 'default'))\n\n                                                               \n19         lo <- floor(n * trim) + 1                           \n20         hi <- n + 1 - lo                                    \n21         x <- sort.int(x, partial = unique(c(lo, hi)))[lo:hi]\n22     }                                                       \n23     .Internal(mean(x))                                      \n24 }                                                           \n\n\nThis shows the inner functionality of the S3 method behind mean. Interestingly, the .Internal at the very end is used to fetch C code which adds functionality to the method.\nS4 formally defines classes, while RC (reference class) binds also the methods and has a very similar approach to classes in C++. In the below code we can see how, differently from S3, an S4 method does not have ellipses in the standardGeneric function since it takes on standardized input invariably.\n\n# Print out an S4 function\nshow\n\nstandardGeneric for \"show\" defined from package \"methods\"\n\nfunction (object) \nstandardGeneric(\"show\")\n<bytecode: 0x7ff9b24ba588>\n<environment: 0x7ff9b3646560>\nMethods may be defined for arguments: object\nUse  showMethods(\"show\")  for currently available ones.\n(This generic function excludes non-simple inheritance; see ?setIs)\n\n\nClasses in R can be defined using the structure() function by doing something like foo <- structure(list(), class = 'my_class') and then from there we can start building methods for the class by specifying functions operating on the structure. A hierarchy of sorts can be visualized as follows, first we have a class, then in S3 methods are below functions, which in turn are separated from the class specification (like class and attributes in python). RC style objects are the ones closer to the object-oriented programming way of C++, here methods and attributes are packaged within the class.\n\n\nCreating an S4 Class\nNow, as an example, we will define a simple S4 class for describing a bank account.\n\n# Define the S4 class with slots\nsetClass('bank_account',\n         representation('user' = 'character',\n                        'balance' = 'numeric'))\n\nIn this way, we are able to access the slots of the class with the @ operator, which is specifically reserved to access S4 class slots. Let’s now create a new method which allows the updating of the class object after a money deposit.\n\n# A method to update a class \nsetGeneric('sum')\n\n[1] \"sum\"\n\nsetMethod('sum', # specify a generic function \n          'bank_account', # signature\n          function(x, deposit) {\n            x@balance <- x@balance + deposit\n          })\n\n# If we call methods on print we should see the new method as well\nshowMethods('sum')\n\nFunction: sum (package base)\nx=\"bank_account\"\n\n\nNow let’s try the new class in action and apply the defined method to an object!\n\n# Create a new bank account with user and balance\nba <- new('bank_account', user = 'Mattia', balance = 100)\n\n# Add a deposit (if this was a Ref Class then updates could happen inplace)\nba@balance <- sum(ba, 200)\n\n# Print out new value\nba@balance\n\n[1] 300\n\n\n\n\nWorking with filesystems\nIn R, we are able to work with files by interacting with the host OS and filesystem in I/O workflows and pipelines. The function read.table() is a built-in which helps the user to read files. In addition, by using write.table() we can save a table object with a specified name and separator. R also provides a specific function to dump() a whole environment image into a variable which can be save, although this is highly inefficient.\nAdditionally the tidyverse, through readr, provides a faster interface to achieve the same I/O management.\nAn interesting binary file format to save objects in R but which can be read by any other programming languages is the one provided by the feather library."
  },
  {
    "objectID": "pages/r_paradigms.html#working-with-data",
    "href": "pages/r_paradigms.html#working-with-data",
    "title": "R Programming Course",
    "section": "Working with Data",
    "text": "Working with Data\n\nVectorized functions\nBase R functions for manipulating basic data structures are many. One of the most potent family of functions in R is the *apply one. In this case we can iterate over elements of a list or columns of a dataframe iteratively by applying a custom function without the need to declare it explicitly outside.\n\n# Use apply over a list\nl = list(   1:5\n          , c(\"a\",\"b\")\n          , c(T,F,T,T) )\n\n# apply length to the list elements\nlapply(l, length)\n\n[[1]]\n[1] 5\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 4\n\n\nThen we can operate over dataframe numerical columns extracting correlation values like the following, using the function pairs().\n\n## quick graphical overview by the scatterplot matrix\nd <- data.frame('id'=runif(100), 'score'=runif(100), 'value'=runif(100))\n\npairs(d[,c(\"id\", \"score\",\"value\")]\n      , lower.panel = panel.smooth\n      , upper.panel = NULL)\n\n\n\n\n\n\n\n\n\n\nOperations on Matrices\nOperations on numerical 2D data like matrices can be operated upon by using a couple of very powerful functions like scale() and sweep().\n\n# Showcase the functions above\nm = matrix(round(runif(9),2),nr=3,nc=3) \n\n# Scale (mean 0 and variance 1)\nscale(m, center = TRUE, scale = TRUE)\n\n           [,1]       [,2]       [,3]\n[1,]  0.9271726 -1.0843803  0.7154837\n[2,] -1.0596259  0.8858318 -1.1426382\n[3,]  0.1324532  0.1985485  0.4271545\nattr(,\"scaled:center\")\n[1] 0.2866667 0.6366667 0.7666667\nattr(,\"scaled:scale\")\n[1] 0.2516611 0.2182506 0.3121431\n\n# Sweep (apply a vector over a matrix summarizing an operation)\n# median value of each row of the matix\nrow.med <- apply(m, MARGIN = 1, FUN = median)\n# subtracting the median value of each row\nsweep(m, MARGIN = 1, STATS = row.med, FUN = \"-\")\n\n      [,1]  [,2] [,3]\n[1,]  0.00 -0.12 0.47\n[2,] -0.39  0.42 0.00\n[3,] -0.36  0.00 0.22"
  },
  {
    "objectID": "pages/r_paradigms.html#the-tidyverse",
    "href": "pages/r_paradigms.html#the-tidyverse",
    "title": "R Programming Course",
    "section": "The Tidyverse",
    "text": "The Tidyverse\nOf course the functionality of many of these functions has been superseded and augmented by the ones provided by the dplyr package, at least in terms of operating over dataframes."
  },
  {
    "objectID": "pages/r_paradigms.html#code-optimization-and-performance-testing-in-r",
    "href": "pages/r_paradigms.html#code-optimization-and-performance-testing-in-r",
    "title": "R Programming Course",
    "section": "Code optimization and performance testing in R",
    "text": "Code optimization and performance testing in R\n\nVariable scoping\nFunctions in R map a repetitive behaviour onto an input and generate a return value. In R, .Primitive is used to call a function from the underlying C language. In functions, scoping refers to how a value is assigned to an environment variable, this will test whether the variable already exists or not and whether the variable exists in a parent or child environment. Scoping is divided in both static and dynamic. In static scoping we can have four basic principles including name masking, by which if a name is not defined inside a function, R will look for it in the parent environment. Another feature is dynamic lookup, by which we can have multiple assignments to the same value along the code.\n\nx <- 1\ny <- 3\n\n# Demonstrate name masking\nfun <- function(){\n  x * y  \n}\n\nfun()\n\n[1] 3\n\n# Demonstrate dynamic look-up\ndl <- function(){\n  x <- x + 1\n}\n\n# first value\nx <- 0\ndl()\n\n# second value\nx <- 2\ndl()\n\n\n\nException handling\nIn the case we end up calling a variable inside a function which does not exists, R will throw an error since it cannot find the variable anywhere in the function or in the parent environment. We can check the existance of variables by using exist(). While defining a function we can check the existance of specific argument using the missing() function inside a function in a conditional which also includes the stopping of the function in case of positive status with stop(). This concept extends to condition handling and recovery with the try() and catch() couple of commands. Additionally, by using tryCatch() we can extend this behaviour further by encapsulating both command, warning and error in the same function which will decide between warning or error.\n\n# tryCatch demonstration\n\nIn R, there are a few functions which help with debugging functions, we can set a debug() function working in order to check the status of a function. Additionally one can use browse() and traceback() to check whether the function is doing the correct thing.\n\n\nProfiling with rbenchmark\nAlso, one can check for function performance and benchmarking with specific libraries in R like rbenchmark::benchmark(). Calling this function, we can manage the avergae execution time and get data from the execution of more than one function altogether.\nOptimizing code to make it faster is an iterative process which essentially entails three steps including finding the bottleneck, trying to eliminate it or substitute it and then repeat the process until the code is “fast enough”. These steps are facilitated by profiling the code, some demonstration is present below.\n\n# Code profiling\n\nThe main thing is to understand how R handles every calculation, this procedure is known as profiling can be achieved in R by different means and packages. We can demonstrate this by applying this workflow to a linear regression setting.\n\n# Linear regression code profiling"
  },
  {
    "objectID": "pages/r_paradigms.html#the-packaging-system-in-r",
    "href": "pages/r_paradigms.html#the-packaging-system-in-r",
    "title": "R Programming Course",
    "section": "The packaging system in R",
    "text": "The packaging system in R\n\nLibraries in r\nPackages in R are installed by using either CRAN or other public package repositories like Bioconductor as we have seen yesterday. R uses both a system-wide access library (admin privileges needed) and a personal library living in the home directory of the user (usually located at /usr/local/bin/R). The tendency is to use only source code within libraries without relaying on third-party code in order to keep everything contained and requiring only compilation of its own source code. RStudio is an IDE completely based and dependant on R which allows developers to start a fresh project from a template directory system. In this case we can set a project name and already select eventually present source files from which to start building. Additionally, all code can be promptly synced with an existing or a new git repository to provide efficient tracking of code changes. The same can be achieved in CLI by running package.skeleton(). Some source of help useful to understand how to build R packages include the official R documentation and Karl Brockman’s Primer (also contains many other interesting tutorials and primers for C++).\n\n\nGenerating function docs\nIn order to automate building effective documentation (which resided in a .Rd file for each function contained in the man directory) and function annotation, which can become a really painful process, we can rely on the Roxygen2 which takes specific annotations in the source code and turns them into function documents respecting the format needed by R. An example of a function with proper Roxygen2 annotations is the following:\n\n# Illustration of Roxygen2 docs\n\n#' A simple and useless function\n#' \n#' This function is just here to print stuff out and demonstrate the use of \\code{roxygen}. \n#' Use \"@param\" to list the paramters of the function with descriptions. \n#' Use \"@return\" to describe the return values of the function. \n#' Use \"@export\" to export the function in namespace or do not use it if the function is supposed to remain of internal use. \n#' \n#' @param x whatever you want to print out.\n#' \n#' @return None\n#' \n#' @export\n\nmy_fun <- function(x, ...){\n  print(x, ...)\n}\n\nAnother important part of R packages is the NAMESPACE, which contains instructions on how to export functions when loading a package. This file will also be generated by Roxygen2 when needed, and the tool will automatically export only the right and needed namespaces (function names).\n\n\nAdding data to packages\nAnother useful feature that a developer can use while creating a package is the insertion of data as well. This data can be used for testing or use as available datasets for showcasing the package functionalities. This can also be really useful in the case of static files which are sourced from functions within the package. This documenting of data can also be handled by Roxygen2 through the creation of another .Rd file. In the data definition we can provide a @docType data and a @usage data to refer to the fact that is data and the way to source it using a function.\n\n# Here's a way to generate some data and store it into a file so that it can be source by a function in the package\n# This code should be saved as any other source code, and with the annotations it will generate automatically a .Rd file with documentation\n\n#' Random data loaded inside a package\n#' \n#' @docType data\n#' \n#' @usage data(random_matrix)\n\nrandom_matrix <- matrix(runif(100), ncol=10)\n\nThe above matrix will then be loaded and included in the worskpace after the package has been loaded and can be called using random_matrix as a variable, the same way that mt_cars is sourced from ggplot’s default datasets."
  },
  {
    "objectID": "pages/prog_base.html",
    "href": "pages/prog_base.html",
    "title": "R Programming Course",
    "section": "",
    "text": "The existence of programming languages is strongly related to how computers work and how they have been developed during their history. Architecture is the base on which languages are built. Computers are built and organized in what is known as Von Neumann architecture, in this sense we have a CPU which is the main component involved in calculations. This is in turn composed of an arithmetic/logic unit a control unit and registers (MAR and MDR) on which data is kept pre-operation after it has been sourced from the memory unit (RAM). In between the memory and the CPU lies the cache (L1, L2) which hosts information that can be retrieved in a faster way than from memory.\n\n\n\n\n\n\nSo, what is a program? Memory and CPUs can only understand numbers, specifically bits of information, 0s and 1s and are only able to perform very simple operations, meanwhile higher-level operations are achieved by using algorithms which compound simpler operations iteratively. The first step towards translating bit instructions into alphanumeric meaningful words was the Assembly language. Assembly worked by using procedure (or routines) calling, we can think of routines as blocks of codes to execute in larger scripts. Part of this process was also involving memory management procedures, with pre-allocated memory (static) to store all global variables in a last-in first-out way and this set the initial mechanism for running programs, other parts of memory include the stack, which is the part related to local variables and programs and then a part of the memory reserved to code (or text in general), called text and finally a part known as heap which is, in line with its name, very dynamic and changes in relation to changes in need at runtime, this part of memory is the one which can be controlled by the programmer (in compiled languages usually) or by programs themselves (in interpreted languages usually) as well, here programs can be restricted in terms of usage. This was mainly engineered to avoid memory conflicts between running programs, so it became crucial to control the system and assign memory to running processes in a consistent and stable manner. This task was tackled by developing operating systems (OS).\n\n\n\nThe OS is instrumental in establishing order in resource and memory management across programs running on the same machine from the same or different users. Another step towards this is to share a machine’s CPU for time-sharing across processes running at the same time, all this is programmed within the OS, which is tasked with managing resources. In the 80s and 90s, with the advent of networking, distributed systems were created in which programs could be run on different machines by taking advantage of web-based protocols (like ssh). After the initial rise of OSs, forms of mass storage were needed in order to have a memory support built into the computer itself, this started with CDs and then hard-drives, which nowadays have been substituted in large by faster technologies such as SSDs and cloud-based solutions for storing data.\n\n\n\nAll along, expressive languages were also developed which were needed to get a more abstract sense of operations and allow for more comprehensible syntax for more complex operations like cycling and conditional execution, introduced a grammar based on data structures, types and objects. Ultimately this allows the programmer to focus on the actual program itself more than its implementation.\nSo summed up, the advantages of languages include: + Loops + Conditional execution + Code blocks + Operators + Data structures (specifications) + Function definition\nObjects in memory are always stored sequentially and specifically, matrices and tabular data in Fortran systems (and the majority of other languages) are stored column-wise, so trying to index a matrix like A[57] == A[7,6] means that the 57th element of a matrix in memory is equal to the 7th element on the 6th column in the R object (which is 1-indexed).\n\nindex <- c(1,2,3,NA)\nrandom <- runif(100)\n\n# Try to use NA as index (should return an NA in that index position)\n# This highlights a fault in R's permessibility, here it does not alert the use for the sake of giving on answer\nrandom[index]\n\n[1] 0.5204957 0.4629572 0.8244709        NA\n\n\n\n\n\nIn the above case we can check that R, an interpreted language, masks the user with the ability to manage an eventual error for the sake of code usability, the user then misses a chance to catch an eventual mistake in the code, maybe while generating the index above, an NA was not wanted, but R does not signal its presence while subsetting random. This mechanisms of silent coercion are widespread in R and the user should always be aware of when they can happen since they can represent a scenarion of bug generation and data loss."
  },
  {
    "objectID": "pages/prog_base.html#parallel-programming",
    "href": "pages/prog_base.html#parallel-programming",
    "title": "R Programming Course",
    "section": "Parallel Programming",
    "text": "Parallel Programming\n\nThe Basics\nWhen programmming something using interpreted (vs compiled, i.e. C++) languages like R, primarily thought as interactive, it is important to stop and understand how functions work in terms of operations and I/O instead of just getting into the thick of it and start using up resources. This is primarily concerning memory requirements when working on a shared platform as well as actual CPU resources. Dynamic vs Static libraries. To provide an example of the performance that can be obtained by programming in compiled languages, let’s imagine a scenario by which we have a series of DNA sequences and we want to get all of the 3-base kmers which are present in each sequence. One relatively straightforward way would be to first generate an empty sequence by kmer matrix and then, after having determined all the possible kmers in the sequences (a dictionary), start counting and indexing any occurrence into the appropriate position. Although this works well with short sequences, the advantages which come from the conjunction of binary encoding and the use of C++ which allows the direct accession of space on memory grant amazing advantages. Brush-up on the definition of a core and a thread? But another way to brute-force achieving speed is to use more cores and threads! In order to take advantage of the presence of multiple cores, we can run different processes by sizing the data into smaller chunks (easier, less efficient) or make a program multithread, this is the best way possible but also the most expensive in terms of implementation time. By multithreading we split up the instructions of a program into spawning different children processes. Parallelization comes with a few challenges, including timing and managing execution I/O across child processes. There are cases in which data needs to be modified by different processes thus we need to ensure that we are not compromising the outputs and/or mixing information from different processes.\n\n\nGraphics Processing Units\nGPUs work very well for linear algebra calculations, especially linear algebra concepts and operations. GPUs are very “dumb” compare to CPUs in terms of the kinds of operations and the complexity of operations that they can do, but what they were initially designed to do (rendering graphics, therefore large matrices) proved also very powerful when applied to operations involving matrices. These cards have a very different architecture as opposed to the Von Neumann one seen previosuly, they contain many more threads doing the same operation on the same data, which needs, crucially, to be copied on the CPU itself, which will also store the final object after computation which will need to be re-imported into memory.\n\n\nOOP History and Paradigms\nProgramming has taken up many shapes during its history, we can have unstructured programming which then evolved into procedural programming by definig re-usable functions which otherwise need repetition. The next step in complexity is represented by modular programming, by which modules of code are utilized and interconnect to provide functionality across different files. The final step is represented by the object-oriented paradigm, by which objects are identified by a set of methods coupled with the needed data, which in turn is strictly related to the methods. Let’s imagine that we want to create a new genomics object in which to store genomic data, this object can be as general as possible, with specific properties and methods. The former might contain IDs from different classification systems, the annotation and the organism of origin while the latter might instead contain actions which can be performed on the object such as alignment, changes in bases or calculation of GC content etc… In contrast to procedural programming, the program flow in OOP is said to be “message oriented”, by this meaning that the user talks to objects by sending messages through methods. The interesting part of OOP is that classes are not just created to be on their own, instead relationships between objects is desirable as a distinguished feature, for example an object representing a transcript can be part of an object representing a chromosome and can inherit actions which can be performed on other elements from the chromosome object, for example determining start and end locations, which could also be use for a genomic sequence, which in turn can be another class under the chromosome one. Methods defined in a parent class are re-implemented in the daughter classes, in this way in the code we can always call the same method and the correct implementation will always be selected since it is specified in the appropriate class, this is called polymorphism."
  }
]