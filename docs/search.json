[
  {
    "objectID": "pages/index.html",
    "href": "pages/index.html",
    "title": "R Programming Course",
    "section": "",
    "text": "Introduction\nThe course will give an overview of R as an instrument for programming and efficient scripting, with a twist on using it to analyze biologically relevant data. It will start with the basics and then develop into an exploration of ways to improve code performance and structure. All of the materials from the lectures can be found here, with relevant information related to the authors as well.\nThe course, which lasted one week, has been taught by Dr. U. Pozzoli, Dr. F. Iannelli and Dr. M. Cereda at IEO in Milan.\nThe statistics page instead contains notes from a separate couse taught by Dr. N. Pesenti at INGM.\nAll the material from the course is either mine or provided by the instructors. All of the code present within this site was developed and is readily implementable in RStudio using RMarkdown."
  },
  {
    "objectID": "pages/r_genomics.html",
    "href": "pages/r_genomics.html",
    "title": "R Programming Course",
    "section": "",
    "text": "Bioconductor was initially created with the idea in mind to generate an open-source place based on the R programming language on which to store packages and projects for the analysis of genomics data. Given the nature of Bioconductor, it encourages contributions and learning through massive documentation bundled with every package. Bioconductor packages are focused on deploying many different functionalities spanning from data visualization for genomics to differential expression analysis, all of this coupled with the added learning bonuses coming from the release of vignettes (accessible with the function vignette()).\n\n\n\nSome of the most used packages in Bioconductor used for genome arithmetics include IRanges and GenomicRanges. Since the genome is a linear one-dimensional coordinate system, we can perform different kinds of operations related for instance to overlaps or differences. For example in the former package, we can define a set of three ranges by using the following code:\n\nlibrary(\"IRanges\")\n\n# Define IRanges object\nmyiranges <- IRanges(start=c(5,20,25), end=c(10,30,40))\n\n# Accessor functions\nstart(myiranges) \n\n[1]  5 20 25\n\n# Ends\nend(myiranges)\n\n[1] 10 30 40\n\n# Lengths (note how we use 0-indexed bases)\nwidth(myiranges)\n\n[1]  6 11 16\n\n\nChecking the class of these objects we would see that they are S4, like all Bioconductor objects. This means that the operations performed above are already stored in slots of the object accessible with the @ operator.\n\nmyiranges@start\n\n[1]  5 20 25\n\n\nIRanges objects can also be represented visually with the following code function.\n\n# Define function to draw IRanges\nillustrate_iranges <- function(obj) {\n    for(i in seq_along(obj))\n        cat(rep(\" \", start(obj)[i]),\n            rep(\"=\", width(obj)[i]),\n            \"\\n\", sep=\"\")\n}\n\nillustrate_iranges( myiranges )\n\n     ======\n                    ===========\n                         ================\n\n\nAn advancement based on the IRanges package is provided by the GRanges one, which provides basically the same interface with more integration with other Bioconductor packages.\nMost of the provided operations above are recalling ones already existant in C-based packages like bedtools.\n\n\n\nAnother way of visualizing genomics elements is through strings, after all DNA sequences are just sets of letters. A useful suite of commands is the one provided by the package Biostrings.\n\nlibrary(Biostrings)\n\n# Define a sequence\nmyseq <- DNAString(\"atggaaaccgcgctgctgatttgcgcgtaa\")\nmyseq\n\n30-letter DNAString object\nseq: ATGGAAACCGCGCTGCTGATTTGCGCGTAA\n\n\nThis provides a multitude of functions that provide the ability to generate for instance reverse complements of the original sequence.\n\n# Determine reverse complement\nreverseComplement(myseq)\n\n30-letter DNAString object\nseq: TTACGCGCAAATCAGCAGCGCGGTTTCCAT\n\n\nThis for instance is useful since in FASTA format files, the genes are always present with their forward strand and therefore we might need the reverse complement if the gene is antisense. This capability is achieved with the DNAString class and its associated methods. We can, for instance, translate a DNA sequence as well.\n\ntranslate(myseq)\n\n10-letter AAString object\nseq: METALLICA*\n\n\nWe are also able to exploit functions to check for the existance of patterns within our sequences.\n\n# Match basic pattern\nmatchPattern(\"ATG\", myseq)\n\nViews on a 30-letter DNAString subject\nsubject: ATGGAAACCGCGCTGCTGATTTGCGCGTAA\nviews:\n      start end width\n  [1]     1   3     3 [ATG]\n\n\nThe fuctionality provided by the DNAString package is expanded and vectorized across multiple sequences thanks to the DNAStringSet which allows us to collect and operate over multiple DNA sequences at once.\nBioconductor already contains many packages which gather genetic information for many model organisms, one of them is BSgenome while others live within the .org family of packages."
  },
  {
    "objectID": "pages/r_genomics.html#genomics-data-visualization",
    "href": "pages/r_genomics.html#genomics-data-visualization",
    "title": "R Programming Course",
    "section": "Genomics data visualization",
    "text": "Genomics data visualization\n\nPlotting basics in R\nOne of the main uses for data visualization is to perform exploratory data analysis (EDA). The basic library in R for creating customisable and clear visualization is ggplot. This package is inserted within a rich environment of data manipulation tools and visualization called tidyverse. The fundamental idea related to data visualization is to communicate a story using the data at our disposal. Relationships hidden in data stand out with proper visualization techniques and ideas. That’s the thing, it’s not only about dull application but also about creativity and storytelling. This phenomenon is especially relevant while working with huge datasets where relationships can exist in many directions and between many variables at once, and where each variable might be represented by thousands or millions of observations. This is exactly the case for genomic data sets describing relevant biological information. Other times, like below, the plotting functionality of R can be adapted to create generative art.\n\n\nShow code\n# Generative art with R example with folded code?\nlibrary(dplyr)\nlibrary(jasmines)\n\n# randomize\nuse_seed(runif(1)) %>%\n  scene_discs(\n    rings = 3, points = 5000, size = 5\n  ) %>%\n  mutate(ind = 1:n()) %>%\n  unfold_warp(\n    iterations = 1,\n    scale = .5, \n    output = \"layer\" \n  ) %>%\n  unfold_tempest(\n    iterations = 20,\n    scale = .01\n  ) %>%\n  style_ribbon(\n    palette = palette_named(\"vik\"),\n    colour = \"ind\",\n    alpha = c(.1,.1),\n    background = \"white\"\n  )\n\n\n\n\n\n\n\n\n\nIn its basic functionality, ggplot exploits what is known as “the grammar of graphics”, gg. This consists of a layered mechanism of instructions. We start by defining which variables of interest are to map to which plot characteristic between color, fill, shape and size. This happens by using the aes() function within either a direct ggplot call or within geoms. geoms are used to define what kind of plot to use, and this of course is going to depend on which variable relationships we want to highlight. One of the paradigms of plotting data is the variable-chart type relationship. By this we mean the right coupling between chart type and appropriate variable nature to plot (i.e. continous vs discrete).\nThese are the main relationships to take into account when plotting:\n\nOne variable, continous: histogram, density curve (distributions)\nOne variable, discrete: bar chart, pie chart (proportions) Two variables, continuous: scatter plot, 2D density kernels (distributions)\nTwo variables, discrete: confusion matrices, heatmaps (common occurence)\nTwo variables, one continuous and one categorical: box plot, violin plot (distributions)\n\nThis, in practice, for a minimal plot between two continuous variables:\n\nlibrary(ggplot2)\nlibrary(RColorBrewer)\n\ndata <- mtcars\n\n# Plot the relationship between a car displacement and mpg, colored by number of cylinders and give a dot size related to the car's weight\n# From this it appears clear how heavier cars have bigger engines, consume more and have more cylinders.\nggplot(data, aes(x = disp, y = mpg, col = as.factor(cyl))) +\n    geom_smooth(color='black') +\n    geom_point(aes(size = wt), alpha=0.8) +\n    scale_color_brewer(palette = 'Paired') +\n    ggtitle('Miles per gallon and Engine Displacement') +\n    theme_classic() \n\n\n\n\n\n\n\n\nObviously there exist many extensions which add functionality to the basic package. For example we can plot maps with map_data (or leaflet widgets) or alluvial diagrams (a subset of Sankey diagrams) with ggalluvial. Another useful package when studying clonal evolution (in tumors particularly) is fishplot, which creates publication-ready plots.\n\n\nPlotting heatmaps\nHeatmaps are visual representations of tabular data. One of the most used packages to create heatmaps is pheatmap and its more personalizable counterpart complexHeatmap.\nThis package allows the user to define heatmap elements as separate classes within a list.\n\n\nShow code\nlibrary(tidyverse)\nlibrary(ComplexHeatmap)\nlibrary(circlize)\n\n# Fetch data\nset.seed(42)\n\nexpr = readRDS(paste0(system.file(package = \"ComplexHeatmap\"), \"/extdata/gene_expression.rds\"))\n\nmat <- expr[1:10, 1:10]\n\n# Turn into matrix\nmat <- as.matrix(mat)\n\n# Draw a matrix with annotations as well\nvalue = rnorm(10)\n\n# Annotation dataframe\ndf = data.frame(type = c(rep(\"a\", 5),\n                         rep(\"b\", 5)),\n                plate = sample(1:20, 10))\n\n# Points\nha = HeatmapAnnotation(df = df,\n                       points = anno_points(value),\n                       col = list( type = c(\"a\" = \"red\", \"b\" = \"blue\"), plate = colorRamp2(c(0, 20), c(\"white\", \"black\"))\n                       ))\n\n# Boxplot\nha_boxplot = HeatmapAnnotation(boxplot = anno_boxplot(mat, axis = TRUE))\n\nHeatmap(mat,\n        name = \"expr\",\n        top_annotation = ha,\n        bottom_annotation = ha_boxplot\n         )\n\n\n\n\n\n\n\n\n\nWe can also check out a more complex heatmap related to measles vaccinations, which also tells a very compelling story.\n\n\nShow code\nmat = readRDS(system.file(\"extdata\", \"measles.rds\", package = \"ComplexHeatmap\"))\n\nha1 = HeatmapAnnotation(\n    dist1 = anno_barplot(\n        colSums(mat), \n        bar_width = 1, \n        gp = gpar(col = \"white\", fill = \"#FFE200\"), \n        border = FALSE,\n        axis_param = list(at = c(0, 2e5, 4e5, 6e5, 8e5),\n            labels = c(\"0\", \"200k\", \"400k\", \"600k\", \"800k\")),\n        height = unit(2, \"cm\")\n    ), show_annotation_name = FALSE)\n\nha2 = rowAnnotation(\n    dist2 = anno_barplot(\n        rowSums(mat), \n        bar_width = 1, \n        gp = gpar(col = \"white\", fill = \"#FFE200\"), \n        border = FALSE,\n        axis_param = list(at = c(0, 5e5, 1e6, 1.5e6),\n            labels = c(\"0\", \"500k\", \"1m\", \"1.5m\")),\n        width = unit(2, \"cm\")\n    ), show_annotation_name = FALSE)\n\nyear_text = as.numeric(colnames(mat))\nyear_text[year_text %% 10 != 0] = \"\"\n\nha_column = HeatmapAnnotation(\n    year = anno_text(year_text, rot = 0, location = unit(1, \"npc\"), just = \"top\")\n)\n\ncol_fun = colorRamp2(c(0, 800, 1000, 127000), c(\"white\", \"cornflowerblue\", \"yellow\", \"red\"))\n\nht_list = Heatmap(mat, name = \"cases\", col = col_fun,\n    cluster_columns = FALSE, show_row_dend = FALSE, rect_gp = gpar(col= \"white\"), \n    show_column_names = FALSE,\n    row_names_side = \"left\", row_names_gp = gpar(fontsize = 8),\n    column_title = 'Measles cases in US states 1930-2001\\nVaccine introduced 1961',\n    top_annotation = ha1, bottom_annotation = ha_column,\n    heatmap_legend_param = list(at = c(0, 5e4, 1e5, 1.5e5), \n        labels = c(\"0\", \"50k\", \"100k\", \"150k\"))) + ha2\n\ndraw(ht_list, ht_gap = unit(3, \"mm\"))\n\ndecorate_heatmap_body(\"cases\", {\n    i = which(colnames(mat) == \"1961\")\n    x = i/ncol(mat)\n    grid.lines(c(x, x), c(0, 1), gp = gpar(lwd = 2, lty = 2))\n    grid.text(\"Vaccine introduced\", x, unit(1, \"npc\") + unit(5, \"mm\"))\n})\n\n\n\n\n\n\n\n\n\n\n\nUpSet plots\nSet operations, particularly useful when dealing with interval genomic data, can be depicted with the use of Venn diagrams, but what it we need more complex operations spanning more than four sets? (the Venn of which can already be quite confusing)\nIn this use case we can check out the package UpSetR which will produce plots line the one below. In this case we can see how it is useful in exploring multiple set relationships all at once. UpSet plots can also be thought of as extensions of heatmaps, by coloring sets by intersection size and annotations.\n\n\noncoPrint\noncoPrint() is a function within the ComplexHeatmap package that became very famous in CNV studies in cancer, these plot are dedicated to checking out gene alterations as matrices where samples are columns and genes are rows.\n\n\nShow code\n# Source from ComplexHeatmap package a heatmap with CNV modifications\nmat = read.table(system.file(\"extdata\", package = \"ComplexHeatmap\", \n    \"tcga_lung_adenocarcinoma_provisional_ras_raf_mek_jnk_signalling.txt\"), \n    header = TRUE, stringsAsFactors = FALSE, sep = \"\\t\")\nmat[is.na(mat)] = \"\"\nrownames(mat) = mat[, 1]\nmat = mat[, -1]\nmat=  mat[, -ncol(mat)]\nmat = t(as.matrix(mat))\n\n# Define graphics\ncol = c(\"HOMDEL\" = \"blue\", \"AMP\" = \"red\", \"MUT\" = \"#008000\")\nalter_fun = list(\n    background = function(x, y, w, h) {\n        grid.rect(x, y, w-unit(2, \"pt\"), h-unit(2, \"pt\"), \n            gp = gpar(fill = \"#CCCCCC\", col = NA))\n    },\n    # big blue\n    HOMDEL = function(x, y, w, h) {\n        grid.rect(x, y, w-unit(2, \"pt\"), h-unit(2, \"pt\"), \n            gp = gpar(fill = col[\"HOMDEL\"], col = NA))\n    },\n    # big red\n    AMP = function(x, y, w, h) {\n        grid.rect(x, y, w-unit(2, \"pt\"), h-unit(2, \"pt\"), \n            gp = gpar(fill = col[\"AMP\"], col = NA))\n    },\n    # small green\n    MUT = function(x, y, w, h) {\n        grid.rect(x, y, w-unit(2, \"pt\"), h*0.33, \n            gp = gpar(fill = col[\"MUT\"], col = NA))\n    }\n)\n\n# Simplify rectangles with alter_graphics()\n# Just for demonstration\nalter_fun = list(\n    background = alter_graphic(\"rect\", fill = \"#CCCCCC\"),   \n    HOMDEL = alter_graphic(\"rect\", fill = col[\"HOMDEL\"]),\n    AMP = alter_graphic(\"rect\", fill = col[\"AMP\"]),\n    MUT = alter_graphic(\"rect\", height = 0.33, fill = col[\"MUT\"])\n)\n\ncolumn_title = \"TCGA Lung Adenocarcinoma, genes in Ras Raf MEK JNK signalling\"\nheatmap_legend_param = list(title = \"Alternations\", at = c(\"HOMDEL\", \"AMP\", \"MUT\"), \n        labels = c(\"Deep deletion\", \"Amplification\", \"Mutation\"))\n\n# Draw oncoPrint\noncoPrint(mat,\n    alter_fun = alter_fun, col = col, \n    remove_empty_columns = TRUE, remove_empty_rows = TRUE,\n    column_title = column_title, heatmap_legend_param = heatmap_legend_param)\n\n\n\n\n\n\n\n\n\n\n\ncBioPortal for Cancer Genomics\nThis is a website hosted by MSKCC where cancer cases can be collected in repositories which can be queried as normal SQL databases to extract relevant metadata and genomic data coming from a great amount of studies. These sources can be accessed via the web and can be downloaded to re-create plots.\n\n\nGenome-level Heatmaps\nHere we are producing genome-level heatmaps divided by chromosome and/or genes. These can be further decorated and transformed into visually pleasing plots by using additional libraries like circlize, which is the R implementation of Circos plots.\n\n\nShow code\nlibrary(circlize)\n\n# Initialize\nset.seed(123)\n\nn = 1000\n\ndf = data.frame(\n    factors = sample(letters[1:8], n, replace = TRUE) # Categories\n  , x = rnorm(n) # Vector\n  , y = runif(n) # Vector\n  )\n\ncircos.par(\"track.height\" = 0.1)\ncircos.par(\"points.overflow.warning\" = FALSE)\n\ncircos.initialize(factors = df$factors, x = df$x)\n\n# Add a track\ncircos.track(\n      factors = df$factors\n    , y = df$y,\n      panel.fun = function(x, y) {\n        circos.text(CELL_META$xcenter,\n                    CELL_META$cell.ylim[2] + uy(5, \"mm\"), \n                    CELL_META$sector.index)\n        # Adding Axes\n        circos.axis(labels.cex = 0.6)\n    }\n)\n\ncol = rep(c(\"#A6B1E1\", \"#424874\"), 4)\n\n# Adding points\ncircos.trackPoints(df$factors, df$x, df$y, col = col, pch = 16, cex = 0.5)\n\n# add sector name outside\ncircos.text(-1, 0.5, \"text\", sector.index = \"a\", track.index = 1)\n\n# Add an histogram\nbgcol = rep(c(\"#A6B1E1\", \"#424874\"), 4)\ncircos.trackHist(df$factors, df$x, bin.size = 0.2, bg.col = 'white', col = bgcol)\n\n# Add links\ncircos.link(\"a\", 0, \"b\", 0, h = 0.4)\n\ncircos.link(\"c\", c(-0.5, 0.5), \"d\", c(-0.5,0.5), col = \"#D6E5E3\", border = \"black\", h = 0.2)\n\ncircos.link(\"e\", 0, \"g\", c(-1,1), col = \"#CACFD6\", border = \"black\", lwd = 2, lty = 2)\n\n\n\n\n\n\n\n\n\nThen once drawn, we can clear the canvas.\n\n# Clear circos environment\ncircos.clear()\n\nInterestingly, we can initialize a circos plot in R already containing genomic coordinates in the form of chromosome ideograms.\n\ncircos.initializeWithIdeogram(species = \"hg19\")\ntext(0, 0, \"Human Chr\", cex = 1)\n\n\n\n\n\n\n\n\nWe also have the ability of plotting transcript sequences, in the following plot we show transcript isoforms for TP53, TP63 and TP73.\n\n\nShow code\n# Map coordinates\ndf = data.frame(\n    name  = c(\"TP53\",  \"TP63\",    \"TP73\"),\n    start = c(7565097, 189349205, 3569084),\n    end   = c(7590856, 189615068, 3652765))\n\n# Initialize\ncircos.genomicInitialize(df)\n\n# Get transcripts and insert them in object\ntp_family = readRDS(system.file(package = \"circlize\", \"extdata\", \"tp_family_df.rds\"))\n\ncircos.genomicInitialize(tp_family)\ncircos.track(ylim = c(0, 1), \n    bg.col = c(\"#FF000040\", \"#00FF0040\", \"#0000FF40\"), \n    bg.border = NA, track.height = 0.05)\n\n# Add rectangles with genes, and transcripts lines\nn = max(tapply(tp_family$transcript, tp_family$gene, function(x) length(unique(x))))\n\ncircos.genomicTrack(tp_family\n                    , ylim = c(0.5, n + 0.5)\n                    , panel.fun = function(region, value, ...) {\n                        all_tx = unique(value$transcript)\n                        for(i in seq_along(all_tx)) {\n                            l = value$transcript == all_tx[i]\n                            # for each transcript\n                            current_tx_start = min(region[l, 1])\n                            current_tx_end = max(region[l, 2])\n                            circos.lines(c(current_tx_start, current_tx_end), \n                                c(n - i + 1, n - i + 1), col = \"#CCCCCC\")\n                            circos.genomicRect(region[l, , drop = FALSE], ytop = n - i + 1 + 0.4, \n                                ybottom = n - i + 1 - 0.4, col = \"orange\", border = NA)\n                        }\n                      }\n                    , bg.border = NA, track.height = 0.4)\n\n\n\n\n\n\n\n\n\nShow code\n# Clear object\ncircos.clear()\n\n\nWe can also plot data and zoom into specific genomic positions or chromosomes.\n\n\nShow code\nextend_chromosomes = function(bed, chromosome, prefix = \"zoom_\") {\n    zoom_bed = bed[bed[[1]] %in% chromosome, , drop = FALSE]\n    zoom_bed[[1]] = paste0(prefix, zoom_bed[[1]])\n    rbind(bed, zoom_bed)\n}\n\ncytoband = read.cytoband()\ncytoband_df = cytoband$df\nchromosome = cytoband$chromosome\n\nxrange = c(cytoband$chr.len, cytoband$chr.len[c(\"chr1\", \"chr2\")])\nnormal_chr_index = 1:24\nzoomed_chr_index = 25:26\n\n# normalize in normal chromsomes and zoomed chromosomes separately\nsector.width = c(xrange[normal_chr_index] / sum(xrange[normal_chr_index]), \n                 xrange[zoomed_chr_index] / sum(xrange[zoomed_chr_index])) \n\n# Initialize ideogram\ncircos.par(start.degree = 90)\ncircos.initializeWithIdeogram(extend_chromosomes(cytoband_df, c(\"chr1\", \"chr2\")), \n    sector.width = sector.width)\n\n# Add track\nbed = generateRandomBed(500)\ncircos.genomicTrack(extend_chromosomes(bed, c(\"chr1\", \"chr2\")),\n    panel.fun = function(region, value, ...) {\n        circos.genomicPoints(region, value, pch = 16, cex = 0.3)\n})\n\n# Add a link from chr1 to the zoomed chr1\ncircos.link(\"chr1\", get.cell.meta.data(\"cell.xlim\", sector.index = \"chr1\"),\n    \"zoom_chr1\", get.cell.meta.data(\"cell.xlim\", sector.index = \"zoom_chr1\"),\n    col = \"#00000020\", border = NA)\n\n\n\n\n\n\n\n\n\nShow code\ncircos.clear()"
  },
  {
    "objectID": "pages/r_paradigms.html",
    "href": "pages/r_paradigms.html",
    "title": "R Programming Course",
    "section": "",
    "text": "In R, we can have beginner data structures (i.e. vectors, matrices and dataframes) and more advanced structures (i.e. S3, S4 and RC). Simpler data structures can further be subset into homogeneous or heterogeneous data structures. Below we can find a representative diagram of data structure relationships in R.\n\n\n\nAtomic vectors and lists are both mono-dimensional data types which differ due to the type of data they can contain, one is formed by elements of the same type, while lists are heterogeneous. Operations based on vectors in R are element-wise, meaning that vector-vector operations are done element by element (with the shorter vector being recycled) while scalar-vector operations broadcast the operation to each single element of a vector. In R, atomic vectors can be created using the vector constructor c().\nThe important thing about vectors is the fact that they can be indexed to extract specific elements and can be concatenated together.\n\n# Vector concatenation and indexing\nvec_a <- c(0,1,2,3,4,5)\nvec_b <- c(28,3,45,2)\nvec_c <- c(0,1)\n\n# Concatenate\nfvec <- c(vec_a, vec_b, vec_c, 67, 9)\n\n# Index vector and extract 3rd element (remember that R is 1-based)\nfvec[3]\n\n[1] 2\n\n\nR is known for being a language with vectorized operations, this is crucial when implementing vector operations since taking advantage of this feature can dramatically speed up code. Some of these functionalities are shown below.\n\n# Vectorized addition with a scalar\nfvec + 3\n\n [1]  3  4  5  6  7  8 31  6 48  5  3  4 70 12\n\n# Vector-vector addition\nfvec + c(rep(2,length(fvec)))\n\n [1]  2  3  4  5  6  7 30  5 47  4  2  3 69 11\n\n# Vector-vector multiplication\nfvec * c(rep(2,length(fvec)))\n\n [1]   0   2   4   6   8  10  56   6  90   4   0   2 134  18\n\n\n\n\n\nOf course when adding together vectors we need to make sure that the elements of the addition are of the same length. But what about the underlying vector representation in R? We can access this information by using the utility function str and access an object’s attributes.\n\n# In R, we can function the structure function to check for the structure of the data \n# This coerces the integer\nv <- c('q', 1, 'a', 2)\n\n# Look at structure\nstr(v)\n\n chr [1:4] \"q\" \"1\" \"a\" \"2\"\n\n# Additionally, we can set attributes to values for variables\n'a_name' <- attr(v, 'my_attribute')\n\n\n\n\nAnother functionally important aspect is to remind ourselves of the existance of approximations in calculations, in this case if we type the following:\n\nx <- (sqrt(2))**2\nx - 2 # Should return 0 but it does not\n\n[1] 4.440892e-16\n\n\nThis happens due to the approximation that the machine provides, and this is why we need to pay a great deal of attention when using conditionals with == 0. This is partially rescued by the use of a function like the one dplyr::near().\nHand in hand with this concept is the one of coercion, by which object get converted from their original class to other classes. This in R happens either implicitly or explicitly.\n\n\n\nCreating sequences of numbers in R is fairly straightforward with many different functions available and we can generate both random sequences, repeated sequences or specific distributions (normal, poisson or others) by specifying the needed parameters.\n\n# Genrate 100 random doubles from 0 to 10\ndbls <- runif(100, min=0, max=10)\n\n# Sample 100 values from a standard normal distribution\ndbls <- rnorm(100, mean=0, sd=1)\n\n# Generate 100 random outcomes from a coin flip using a Bernoullu trial\ndbls <- rbinom(100, size=1, prob=0.5)\n\n# Sample 100 values from a negative binomial distribution with size and prob values\ndbls <- rnbinom(100, size=1, prob=0.75)\n\n\n\n\nTwo-dimensional data in R is represented by the Matrix class and can be constructed using the following code.\n\n# Create a matrix\nmat <- matrix(runif(60), ncol=3)\n\n# Assign different dimensions\ndim(mat) <- c(3,20) # 3 rows 20 cols\n\n# Isolate the upper triangle of the matrix in index form\nidxs <- upper.tri(mat, diag = FALSE)\n\nMatrices can be also manipulated to get their diagonal and triangles by using dedicated functions.\n\n# Generate a matrix\nm <- matrix(runif(10), ncol = 5)\n\n# Diagonal \ndiag(m)\n\n[1] 0.5280932 0.5663483\n\n# Upper triangle\nupper.tri(m, diag = TRUE)\n\n      [,1] [,2] [,3] [,4] [,5]\n[1,]  TRUE TRUE TRUE TRUE TRUE\n[2,] FALSE TRUE TRUE TRUE TRUE\n\n\nAdditionally, we can exploit functions which combine vectors to generate matrices.\n\nx <- c(0,2,4,5)\ny <- c(2,5,6,4)\n\n# bind vector into matrix as rows\nrbind(x, y)\n\n  [,1] [,2] [,3] [,4]\nx    0    2    4    5\ny    2    5    6    4\n\n# bind as columnn\ncbind(x, y)\n\n     x y\n[1,] 0 2\n[2,] 2 5\n[3,] 4 6\n[4,] 5 4\n\n\n\n\n\nIn R, matrix-matrix operations are run element-wise, so they have to happen between uniform and consistent, meaning of the same dimensions. Expanding on operations, matrix multiplication in R is achieved with the operator %*% as such:\n\n\n\n\n# Create matrices\nx <- matrix(runif(10), ncol = 5)\ny <- matrix(runif(5), nrow = 5)\n\n# rows by col matrix multiplication\nmatmul <- x %*% y\n\nmatmul\n\n          [,1]\n[1,] 0.6986138\n[2,] 1.0878833\n\n\n\n\n\nLists are probably the most programmatically complex objects in R, they contain different types and have different lenght with or without named elements. They represent the very first interface to Object-oriented programming. Lists are tricky to work with when mixing data types since they can include silent coercion."
  },
  {
    "objectID": "pages/r_paradigms.html#object-oriented-oo-data-type",
    "href": "pages/r_paradigms.html#object-oriented-oo-data-type",
    "title": "R Programming Course",
    "section": "Object-Oriented (OO) data type",
    "text": "Object-Oriented (OO) data type\n\nClasses in general\nHere we are moving from basic structures to objects which can contain data and are associated to methods which are able to operate on the data itself. It is a mode of operation borrowed from C++. A class contains methods and attributes, the whole creates one of the main instances of OOP. Class elements can inherit from other objects which share the same parent. The main thing to understand is that R, by default, treats everything as an object. Everything, literally everything, has attributes and maybe even methods, from basic vectors to complex classes.\n\nvec <- c(1,2,3)\n\n# Illustrate classes\nclass(vec)\n\n[1] \"numeric\"\n\n# Coerce vector to another class as well, so that methods designed with that class im mind, can be applied to this object as well\nclass(vec) <- append(class(vec), 'SPC')\n\n# Now vec will have both classes listed\nclass(vec)\n\n[1] \"numeric\" \"SPC\"    \n\n\nThe way we can think of classes in R is that they are description of things in a specific methods system and are defined by the setClass() function in the methods package (in S4). An object is an instance of a class, these can be created using the new() function. Finally a method is a function operating only on a certain class of objects, in this sense a generic function is one which dispatches methods, this means that they do not perform any computation, they just figure out the class of the data and matches it with an appropriate method. Generics can of course be created and a developer might create associated methods. For S3 system objects there is also a methods function which lets the user explore methods implemented for the function.\n\n\nClass systems\nBy default, R has three object oriented systems, one is S3, it implements classes without defining them strictly but instead defines functions and methods separately across classes. The main functional aspect related to S3 is the ability to overload functions, this means that one function can be swiftly applied to different class elements and behave correctly. S3 functions are by design split into generics and methods and are defined in the code by using generic.class.\n\n# Example of an S3 defined (generic) function is print\nprint\n\nfunction (x, ...) \nUseMethod(\"print\")\n<bytecode: 0x7f9c5f86cdd8>\n<environment: namespace:base>\n\n# Call methods function on a generic function, in this case mean\nmethods('mean')\n\n[1] mean.Date     mean.default  mean.difftime mean.POSIXct  mean.POSIXlt \n[6] mean.quosure*\nsee '?methods' for accessing help and source code\n\n\nS3 pre-dates S4 both in terms of time and functionality, S3 methods system is less rigorous and informal, this called for a more formalized approach tackled by S4.\nWe can explicitly access the code behind an S3 method definition by doing the following:\n\nhead(getS3method('mean', 'default'))\n\n                                                                     \n1 function (x, trim = 0, na.rm = FALSE, ...)                         \n2 {                                                                  \n3     if (!is.numeric(x) && !is.complex(x) && !is.logical(x)) {      \n4         warning(\"argument is not numeric or logical: returning NA\")\n5         return(NA_real_)                                           \n6     }                                                              \n\ntail(getS3method('mean', 'default'))\n\n                                                               \n19         lo <- floor(n * trim) + 1                           \n20         hi <- n + 1 - lo                                    \n21         x <- sort.int(x, partial = unique(c(lo, hi)))[lo:hi]\n22     }                                                       \n23     .Internal(mean(x))                                      \n24 }                                                           \n\n\nThis shows the inner functionality of the S3 method behind mean. Interestingly, the .Internal at the very end is used to fetch C code which adds functionality to the method.\nS4 formally defines classes, while RC (reference class) binds also the methods and has a very similar approach to classes in C++. In the below code we can see how, differently from S3, an S4 method does not have ellipses in the standardGeneric function since it takes on standardized input invariably.\n\n# Print out an S4 function\nshow\n\nstandardGeneric for \"show\" defined from package \"methods\"\n\nfunction (object) \nstandardGeneric(\"show\")\n<bytecode: 0x7f9c5dbb70c0>\n<environment: 0x7f9c5c2fbe70>\nMethods may be defined for arguments: object\nUse  showMethods(\"show\")  for currently available ones.\n(This generic function excludes non-simple inheritance; see ?setIs)\n\n\nClasses in R can be defined using the structure() function by doing something like foo <- structure(list(), class = 'my_class') and then from there we can start building methods for the class by specifying functions operating on the structure. A hierarchy of sorts can be visualized as follows, first we have a class, then in S3 methods are below functions, which in turn are separated from the class specification (like class and attributes in python). RC style objects are the ones closer to the object-oriented programming way of C++, here methods and attributes are packaged within the class.\n\n\nCreating an S4 Class\nNow, as an example, we will define a simple S4 class for describing a bank account.\n\n# Define the S4 class with slots\nsetClass('bank_account',\n         representation('user' = 'character',\n                        'balance' = 'numeric'))\n\nIn this way, we are able to access the slots of the class with the @ operator, which is specifically reserved to access S4 class slots. Let’s now create a new method which allows the updating of the class object after a money deposit.\n\n# A method to update a class \nsetGeneric('sum')\n\n[1] \"sum\"\n\nsetMethod('sum', # specify a generic function \n          'bank_account', # signature\n          function(x, deposit) {\n            x@balance <- x@balance + deposit\n          })\n\n# If we call methods on print we should see the new method as well\nshowMethods('sum')\n\nFunction: sum (package base)\nx=\"bank_account\"\n\n\nNow let’s try the new class in action and apply the defined method to an object!\n\n# Create a new bank account with user and balance\nba <- new('bank_account', user = 'Mattia', balance = 100)\n\n# Add a deposit (if this was a Ref Class then updates could happen inplace)\nba@balance <- sum(ba, 200)\n\n# Print out new value\nba@balance\n\n[1] 300\n\n\n\n\nWorking with filesystems\nIn R, we are able to work with files by interacting with the host OS and filesystem in I/O workflows and pipelines. The function read.table() is a built-in which helps the user to read files. In addition, by using write.table() we can save a table object with a specified name and separator. R also provides a specific function to dump() a whole environment image into a variable which can be save, although this is highly inefficient.\nAdditionally the tidyverse, through readr, provides a faster interface to achieve the same I/O management.\nAn interesting binary file format to save objects in R but which can be read by any other programming languages is the one provided by the feather library."
  },
  {
    "objectID": "pages/r_paradigms.html#working-with-data",
    "href": "pages/r_paradigms.html#working-with-data",
    "title": "R Programming Course",
    "section": "Working with Data",
    "text": "Working with Data\n\nVectorized functions\nBase R functions for manipulating basic data structures are many. One of the most potent family of functions in R is the *apply one. In this case we can iterate over elements of a list or columns of a dataframe iteratively by applying a custom function without the need to declare it explicitly outside.\n\n# Use apply over a list\nl = list(   1:5\n          , c(\"a\",\"b\")\n          , c(T,F,T,T) )\n\n# apply length to the list elements\nlapply(l, length)\n\n[[1]]\n[1] 5\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 4\n\n\nThen we can operate over dataframe numerical columns extracting correlation values like the following, using the function pairs().\n\n## quick graphical overview by the scatterplot matrix\nd <- data.frame('id'=runif(100), 'score'=runif(100), 'value'=runif(100))\n\npairs(d[,c(\"id\", \"score\",\"value\")]\n      , lower.panel = panel.smooth\n      , upper.panel = NULL)\n\n\n\n\n\n\n\n\n\n\nOperations on Matrices\nOperations on numerical 2D data like matrices can be operated upon by using a couple of very powerful functions like scale() and sweep().\n\n# Showcase the functions above\nm = matrix(round(runif(9),2),nr=3,nc=3) \n\n# Scale (mean 0 and variance 1)\nscale(m, center = TRUE, scale = TRUE)\n\n           [,1]       [,2]       [,3]\n[1,] -1.0653416 -1.0866224 -0.7258662\n[2,]  0.9183979  0.2050231  1.1406469\n[3,]  0.1469437  0.8815993 -0.4147807\nattr(,\"scaled:center\")\n[1] 0.2566667 0.5166667 0.3966667\nattr(,\"scaled:scale\")\n[1] 0.09073772 0.16258331 0.16072751\n\n# Sweep (apply a vector over a matrix summarizing an operation)\n# median value of each row of the matix\nrow.med <- apply(m, MARGIN = 1, FUN = median)\n# subtracting the median value of each row\nsweep(m, MARGIN = 1, STATS = row.med, FUN = \"-\")\n\n      [,1] [,2] [,3]\n[1,] -0.12 0.06 0.00\n[2,] -0.21 0.00 0.03\n[3,] -0.06 0.33 0.00"
  },
  {
    "objectID": "pages/r_paradigms.html#the-tidyverse",
    "href": "pages/r_paradigms.html#the-tidyverse",
    "title": "R Programming Course",
    "section": "The Tidyverse",
    "text": "The Tidyverse\nOf course the functionality of many of these functions has been superseded and augmented by the ones provided by the dplyr package, at least in terms of operating over dataframes."
  },
  {
    "objectID": "pages/r_paradigms.html#code-optimization-and-performance-testing-in-r",
    "href": "pages/r_paradigms.html#code-optimization-and-performance-testing-in-r",
    "title": "R Programming Course",
    "section": "Code optimization and performance testing in R",
    "text": "Code optimization and performance testing in R\n\nVariable scoping\nFunctions in R map a repetitive behaviour onto an input and generate a return value. In R, .Primitive is used to call a function from the underlying C language, an example of this is sum(), as seen below.\n\nsum\n\nfunction (..., na.rm = FALSE)  .Primitive(\"sum\")\n\n\nIn functions, scoping refers to how a value is assigned to an environment variable, this will test whether the variable already exists or not and whether the variable exists in a parent or child environment. More specifically, coping is the set of rules that control the way R picks up the value of a variable Scoping is divided in both static and dynamic. In static scoping we can have four basic principles including name masking, by which if a name is not defined inside a function, R will look for it in the parent environment. Functions and variables since the same rules apply if a function is defined within another function. Another feature is dynamic lookup, by which we can have multiple assignments to the same value along the code since static scoping determines where to look for values and not when. This means that R looks for values when a function is run and not when it is created, which in turn means that the output of a function can be different depending on objects outside its environment.\n\nx <- 1\ny <- 3\n\n# Demonstrate name masking\nfun <- function(){\n  x * y  \n}\n\nfun()\n\n[1] 3\n\n# Demonstrate dynamic look-up\ndl <- function(){\n  x <- x + 1\n}\n\n# first value\nx <- 0\ndl()\n\n# second value\nx <- 2\ndl()\n\n\n\nException handling\nIn the case we end up calling a variable inside a function which does not exists, R will throw an error since it cannot find the variable anywhere in the function or in the parent environment. We can check the existance of variables by using exist() while the correct passing of an argument can be checked with the missing() function within an if statement. This can be coupled with a stop() function in order to stop code execution.\n\n# Missing check + Stop check\nfoo = function(x,y,...){\n  if (missing(y)){\n    \n    # STOP\n    stop(\"y is not specified, please STOP\\n\")\n    \n    }else{\n     print(x)\n     print(y)\n     args = list(...)\n     if(\"z\" %in% names(args)) print(args$z)\n  }\n}\n\nThis concept extends to condition handling and recovery with the try() and catch() couple of commands. Additionally, by using tryCatch() we can extend this behaviour further by encapsulating both command, warning and error in the same function which will decide between warning or error and eventually return custom error messages.\n\n# tryCatch demonstration\nfoo = function(z,\n               warning = function(w) {\n                 print( paste('warning:',w) );\n                 },\n                error = function(e) {\n                 print(paste('error:',e));\n               }\n               \n               ){\n        tryCatch(\n          { \n          print(paste(\"attempt log operation for z:\",z))\n          return(log(z))\n          }\n          ,warning = warning\n          ,error = error )\n  }\n\n# Execution\nfoo(2)\n\n[1] \"attempt log operation for z: 2\"\n\n\n[1] 0.6931472\n\n\nThe functionality of tryCatch() can be coupled with program re-starts in order to be able to run the function again with the substitution of input values gracefully. This is achieved by combining tryCatch(), invokeRestart() and withRestarts(). Sample code for this behaviour is found below.\n\nfoo = function(z,\n               \n               ## WARNING FUNCTION with restart\n               warning = function(w) {\n                 print( paste('warning:',w) );\n                 invokeRestart(\"correctArgForWarnings\")\n                 },\n               \n               ## ERROR FUNCTION with restart\n               error = function(e) {\n                 print(paste('error:',e));\n                 invokeRestart(\"correctArgForErrors\")\n               }\n               \n               ){\n  \n  ## Loop is repeated until a break is specified\n  repeat \n    ## 1. catch errors *********************\n    withRestarts(   \n     \n      ## 2. catch warnings =================\n      withRestarts(\n        \n        ## TRY CATCH BLOCk -----------------\n        tryCatch(\n          { \n          print(paste(\"attempt log operation for z:\",z))\n          return(log(z))\n          } # return break the repeat loop\n          ,warning = warning\n          ,error = error )\n          ##------------------------------------\n        \n        , correctArgForWarnings = function() {z <<- -z} ) \n      ##=================================\n      \n      , correctArgForErrors = function() {z <<- 1})\n       ##*********************************\n}\n\nfoo(2)\n\n[1] \"attempt log operation for z: 2\"\n\n\n[1] 0.6931472\n\n\nNow let’s try with an invalid value for a logarithm.\n\n# invokes the warning’s handler\nfoo(-2)\n\n[1] \"attempt log operation for z: -2\"\n[1] \"warning: simpleWarning in log(z): NaNs produced\\n\"\n[1] \"attempt log operation for z: 2\"\n\n\n[1] 0.6931472\n\n\n\n\nDebugging\nIn R, there are a few functions which help with debugging functions, we can set a debug() function working in order to check the status of a function. Additionally one can use browser() to interrupt execution and check the environment at a specific timepoint or traceback() to print the call stack of the last uncaught error.\n\n\nProfiling with rbenchmark\nAlso, one can check for function performance and benchmarking with specific libraries in R like rbenchmark::benchmark(). Calling this function, we can manage the avergae execution time and get data from the execution of more than one function altogether.\nOptimizing code to make it faster is an iterative process which essentially entails three steps including finding the bottleneck, trying to eliminate it or substitute it and then repeat the process until the code is “fast enough”. These steps are facilitated by profiling the code, some demonstration is present below.\nAs an example, we can demonstrated the profiling of a recursive function which calculates a factorial value from an integer. This can be implemented in two ways, either by taking advantage of recursion or by exploiting iteration with for loops.\n\n# Recursion vs Iteration\nfact.recursive = function(n){\n  ifelse (n==1, 1, (n * fact.recursive(n-1) ) )\n}\n\nfact.it = function(n){\n  ans = 1\n  for (ii in 2:n) ans = ans * ii\n  ans\n}\n\nThe simpler way to obtain a benchmark is to check the elapsed time for the execution of the two functions.\n\n# Recursive\nsystem.time(fact.recursive(100))[\"elapsed\"]\n\nelapsed \n  0.003 \n\n\nAnd now the iteration-based one.\n\n# Recursive\nsystem.time(fact.recursive(100))[\"elapsed\"]\n\nelapsed \n      0 \n\n\nThe main thing is to understand how R handles every calculation, this procedure is known as profiling can be achieved in R by different means and packages. We can demonstrate this by applying this workflow to a linear regression setting while using the package rbenchmark.\n\nlibrary(rbenchmark)\n\n# benchmark() is a simple wrapper around system.time()\nbenchmark( fact.recursive(15)\n          , fact.it(15)\n          \n          , order=\"relative\"\n          , replications=5000\n          )\n\n                test replications elapsed relative user.self sys.self\n2        fact.it(15)         5000   0.011    1.000     0.010    0.000\n1 fact.recursive(15)         5000   0.140   12.727     0.137    0.003\n  user.child sys.child\n2          0         0\n1          0         0"
  },
  {
    "objectID": "pages/r_paradigms.html#the-packaging-system-in-r",
    "href": "pages/r_paradigms.html#the-packaging-system-in-r",
    "title": "R Programming Course",
    "section": "The packaging system in R",
    "text": "The packaging system in R\n\nLibraries in r\nPackages in R are installed by using either CRAN or other public package repositories like Bioconductor as we have seen yesterday. R uses both a system-wide access library (admin privileges needed) and a personal library living in the home directory of the user (usually located at /usr/local/bin/R). The tendency is to use only source code within libraries without relaying on third-party code in order to keep everything contained and requiring only compilation of its own source code. RStudio is an IDE completely based and dependant on R which allows developers to start a fresh project from a template directory system. In this case we can set a project name and already select eventually present source files from which to start building. Additionally, all code can be promptly synced with an existing or a new git repository to provide efficient tracking of code changes. The same can be achieved in CLI by running package.skeleton(). Some source of help useful to understand how to build R packages include the official R documentation and Karl Brockman’s Primer (also contains many other interesting tutorials and primers for C++).\n\n\nGenerating function docs\nIn order to automate building effective documentation (which resided in a .Rd file for each function contained in the man directory) and function annotation, which can become a really painful process, we can rely on the Roxygen2 which takes specific annotations in the source code and turns them into function documents respecting the format needed by R. An example of a function with proper Roxygen2 annotations is the following:\n\n# Illustration of Roxygen2 docs\n\n#' A simple and useless function\n#' \n#' This function is just here to print stuff out and demonstrate the use of \\code{roxygen}. \n#' Use \"@param\" to list the paramters of the function with descriptions. \n#' Use \"@return\" to describe the return values of the function. \n#' Use \"@export\" to export the function in namespace or do not use it if the function is supposed to remain of internal use. \n#' \n#' @param x whatever you want to print out.\n#' \n#' @return None\n#' \n#' @export\n\nmy_fun <- function(x, ...){\n  print(x, ...)\n}\n\nAnother important part of R packages is the NAMESPACE, which contains instructions on how to export functions when loading a package. This file will also be generated by Roxygen2 when needed, and the tool will automatically export only the right and needed namespaces (function names).\n\n\nAdding data to packages\nAnother useful feature that a developer can use while creating a package is the insertion of data as well. This data can be used for testing or use as available datasets for showcasing the package functionalities. This can also be really useful in the case of static files which are sourced from functions within the package. This documenting of data can also be handled by Roxygen2 through the creation of another .Rd file. In the data definition we can provide a @docType data and a @usage data to refer to the fact that is data and the way to source it using a function.\n\n# Here's a way to generate some data and store it into a file so that it can be source by a function in the package\n# This code should be saved as any other source code, and with the annotations it will generate automatically a .Rd file with documentation\n\n#' Random data loaded inside a package\n#' \n#' @docType data\n#' \n#' @usage data(random_matrix)\n\nrandom_matrix <- matrix(runif(100), ncol=10)\n\nThe above matrix will then be loaded and included in the worskpace after the package has been loaded and can be called using random_matrix as a variable, the same way that mt_cars is sourced from ggplot’s default datasets."
  },
  {
    "objectID": "pages/r_paradigms.html#web-applications-with-shiny",
    "href": "pages/r_paradigms.html#web-applications-with-shiny",
    "title": "R Programming Course",
    "section": "Web applications with Shiny",
    "text": "Web applications with Shiny\n\nStatic vs Dynamic websites\nStatic websites are called like so since they are deployd but they cannot interact with users in terms of data flow and server requests. Dynamic sites instead allow interactions by using an infrastructure based on frontend and backend structures. An example of static page is provided in the below diagram.\n\n\n\nAn example of a dynamic webpage is provided in the below diagram, notice the presence of server-side scripting capability.\n\n\n\nOne of the most used wrappers for interactive site building in R is Shiny.\n\n\nShiny apps\nShiny apps work by separating UI and Server (a live R session) which end up communicating UI requests happening in the frontend to the backend engine. In Shiny, this crosstalk happens via the creation of a Fluid Page, which is a page type that resizes itself while browsing around. In doing this, Shiny defines a layout that can vary based on the size of the page in a reactive way. In practice, this is implemented via two files, a ui.R (user interface) and a server.R (backend). The UI is effectively an HTML interface which in Shiny is mediated by Bootstrap, which is the most popular HTML, CSS, and JS framework for developing responsive, mobile first projects on the web.\n\n\nHTML tags\nWhat Shiny does behind the scenes is to port HTML into R by wrapping up HTML tag creation with R function calls (110 to be exact, as many as HTML tags). The main ones for structural design are panel functions whithin which we can group different elements. Panels can be controlled in terms of layout by using layout functions acting within the FluidPage() one. The code below is representative of ui.R.\n\n# An example\nlibrary(shiny)\n\nfluidPage( # Fluid page\n  \n  titlePanel(\"A Shiny App!\"),\n  \n  sidebarLayout(\n    \n    sidebarPanel(HTML('<p>\n         <label>A numeric input:</label><br /> \n         <input type=\"number\" name=\"n\" value=\"0\" min=\"1\" max=\"30\" />\n         </p>')),\n   \n     mainPanel(\n      p(strong(\"bold font \"), em(\"italic font\")),\n      p(code(\"code block\")),\n      a(href=\"http://www.google.com\", \"link to Google\"))\n  )\n)\n\n\nA Shiny App!\n\n\n\n         A numeric input: \n         \n         \n\n\n\nbold font \nitalic font\n\n\ncode block\n\nlink to Google\n\n\n\n\n\n\n\nInteractivity with Widgets\nIn order to create a dynamic and interactive page in Shiny we can use Widgets. Once a widget is defined, it needs to know its imputs and outputs as lists, as well as their location in the server side. An important thing is that server-side inputs should be read-only, if inputs are actively modified, then the page will throw an error. There will always a unique correspondance between input and output in Shiny’s reactive programming paradigms since both of them are linked and named across functions."
  },
  {
    "objectID": "pages/statistics.html",
    "href": "pages/statistics.html",
    "title": "R Programming Course",
    "section": "",
    "text": "In this section, we will focus on descriptive statistics and indices, and how these relate to the characteristics of numerical distributions.\n\n\nFirst of all, distributions are collections of data points from a given variable which are sampled from a population of interest.\n\n\n\nThe types of variables we can encounter are diverse. We can have categorical and numerical variables, the former can be ordinal as well, while the latter can either be continuous or discrete. In the following dataset, named cv_dis, we have 3 variables of interest relating to pressures, summarized in 6 columns. The variables of interest are the kind of pressure (sys vs dia) and the time point at which the measure was taken. We have both categorical and continuous variables in cv_dis, and some of the columns need to be converted into factors, since they are originally recognized as dbl, so numeric, while we know that they represent categories encoded in integer numbers.\n\n# Manipulate data\ncv_dis_operated <- cv_dis %>%\n  pivot_longer(names_to = \"bloodp_visit\", values_to = \"pressure\", sys_1:dia_3) %>% # Pivot on pressures\n  separate(bloodp_visit, c(\"type_p\", \"visit\"), sep = \"_\") %>% # separate type of pressure and number of visit in two columns\n  pivot_wider(names_from = \"type_p\", values_from = \"pressure\") # pivot wider to gather the diastolic and systolic pressure as two separate columns\n\n# Now bind the first dataset to other patients from a different dataset\ncv_dis_binded <- cv_dis %>% bind_rows(cv_dis_2)\n\n# Join additional clinical information\ncv_dis_joined <- cv_dis %>% full_join(cv_dis_3, by=c('pat_id'='patient'))\n\nNow we can move onto descriptive statistics and the use of indices to describe data.\n\n\n\n\n# Mean (overestimates many times)                                  - mean(Variable)\n# Geometric mean (for avg growth rates and logged data) - prod(Variable)^(1/(n()-sum(is.na(Variable))))\n# Harmonic mean (for avg ratios)                        - 1/mean(1/Variable)\n# Median (splits the distribution in 2 equal parts)     - median(Variable)\n\n\n\n\n\n# Quantiles (quartile and IQR)                            - quantile(Variable, probs = 0.25)  --> percentile 25th\n# Min                                                     - min(Variable)\n# Max                                                     - max(Variable)\n# Variance (uses the arithmetic mean)                     - var(Variable)\n# Std Dev (uses the arithmetic mean)                      - sd(Variable)\n# Coefficient of var (unitless, comparable to each other) - sd(Variable) / mean(Variable)\n\nExcluding missing values from analyses (one possible way) na.rm = TRUE (ex. mean(Variable, na.rm = TRUE)). Data collection for statistics happens in samples coming from a bigger population, so the data is always a representative piece of information coming from the population. The coefficient of variation can be extremely useful to compare variability across different kinds of variables. It is clear that since these indices are related to the arithmetic mean, all of them are influenced by the validity of the arithmetic mean on the distribution itself. This means that the arithmetic mean, and the associated dispersion indices, need to be used carefully when analyzing data distributions, since they might not be very representiative (for example in the case of heavily skewed distributions)."
  },
  {
    "objectID": "pages/statistics.html#missing-data",
    "href": "pages/statistics.html#missing-data",
    "title": "R Programming Course",
    "section": "Missing data",
    "text": "Missing data\n\nHandling missing data\nIn this section, we are going to focus on handling missing data, which can be a problem in R for different reasons, first of all it invalidates any kind of statistical analysis while propagating errors in code given silent handling habits in R. We start by loading the finalfit package. This package will provide very useful function to have a look at the data and assess the status of the variables.\n\n# Translate some of the variables in the dataset to factors since they are char or dbl\ncv_dis <- cv_dis %>% mutate_at(vars(gender, chol,  gluc, cardio), as.factor)\n\n# Sum up the condition of the dataset by using the finalfit functionality\nglimpse_df <- cv_dis %>% ff_glimpse()\n\n# This returns a list with 2 dfs, one for categorical and one for continous variables\nglimpse_df\n\n$Continuous\n        label var_type   n missing_n missing_percent  mean    sd   min\npat_id pat_id    <dbl> 634         0             0.0 359.9 208.9   1.0\nage       age    <dbl> 634         0             0.0  54.0   9.6  39.0\nheight height    <dbl> 608        26             4.1 164.2   8.0 130.0\nweight weight    <dbl> 603        31             4.9  73.6  13.9  40.0\nsys_1   sys_1    <dbl> 634         0             0.0 125.1  16.7  78.9\nsys_2   sys_2    <dbl> 634         0             0.0 126.3  16.6  80.0\nsys_3   sys_3    <dbl> 557        77            12.1 127.7  16.7  83.8\ndia_1   dia_1    <dbl> 634         0             0.0  79.8  10.0  21.4\ndia_2   dia_2    <dbl> 634         0             0.0  81.0   9.5  60.0\ndia_3   dia_3    <dbl> 557        77            12.1  82.3   9.6  58.7\n       quartile_25 median quartile_75   max\npat_id       176.2  360.0       540.8 720.0\nage           49.0   54.0        59.0 162.0\nheight       158.8  165.0       170.0 187.0\nweight        65.0   72.0        81.0 123.0\nsys_1        117.0  120.3       136.2 207.3\nsys_2        120.0  120.0       140.0 210.0\nsys_3        118.8  123.1       137.7 212.6\ndia_1         75.6   79.1        86.5 122.9\ndia_2         80.0   80.0        90.0 120.0\ndia_3         78.5   81.5        88.2 124.8\n\n$Categorical\n        label var_type   n missing_n missing_percent levels_n        levels\ngender gender    <fct> 634         0             0.0        2      \"F\", \"M\"\nchol     chol    <fct> 634         0             0.0        3 \"1\", \"2\", \"3\"\ngluc     gluc    <fct> 634         0             0.0        3 \"1\", \"2\", \"3\"\ncardio cardio    <fct> 634         0             0.0        2      \"0\", \"1\"\n       levels_count   levels_percent\ngender     403, 231           64, 36\nchol    478, 74, 82       75, 12, 13\ngluc    530, 47, 57 83.6,  7.4,  9.0\ncardio     314, 320           50, 50\n\n\nThis function is really useful to summarize the data, it conveniently highlights missing values as well. In the continuous values dataframe we can also find additional information related to indices and range of the continuous data. Doing this, we can immediately spot the fact that the maximum age value is 162, which of course is wrong (below).\n\nglimpse_df$Continuous[,c('min','max','missing_n','missing_percent')]\n\n         min   max missing_n missing_percent\npat_id   1.0 720.0         0             0.0\nage     39.0 162.0         0             0.0\nheight 130.0 187.0        26             4.1\nweight  40.0 123.0        31             4.9\nsys_1   78.9 207.3         0             0.0\nsys_2   80.0 210.0         0             0.0\nsys_3   83.8 212.6        77            12.1\ndia_1   21.4 122.9         0             0.0\ndia_2   60.0 120.0         0             0.0\ndia_3   58.7 124.8        77            12.1\n\n\nNow, the finalfit package allows us to also plot missing values. In this case we can show where the missing values are located in the dataset, check for alignment of missing values within the data if we expect so (like if a there is a missing value for systolic pressure at a specific visit, then we also expect the diastolic pressure for that visit too) or check whether something unexpectd in missing values is happening. The missing_pattern() function instead allows us to understand which variables are more related in terms of missing values.\n\ncv_dis %>% missing_plot()\n\n\n\n\n\n\n\ncv_dis %>% missing_pattern()\n\n\n\n\n\n\n\n\n    pat_id age gender chol gluc cardio sys_1 sys_2 dia_1 dia_2 height weight\n510      1   1      1    1    1      1     1     1     1     1      1      1\n74       1   1      1    1    1      1     1     1     1     1      1      1\n23       1   1      1    1    1      1     1     1     1     1      1      0\n1        1   1      1    1    1      1     1     1     1     1      1      0\n18       1   1      1    1    1      1     1     1     1     1      0      1\n1        1   1      1    1    1      1     1     1     1     1      0      1\n6        1   1      1    1    1      1     1     1     1     1      0      0\n1        1   1      1    1    1      1     1     1     1     1      0      0\n         0   0      0    0    0      0     0     0     0     0     26     31\n    sys_3 dia_3    \n510     1     1   0\n74      0     0   2\n23      1     1   1\n1       0     0   3\n18      1     1   1\n1       0     0   3\n6       1     1   2\n1       0     0   4\n       77    77 211"
  },
  {
    "objectID": "pages/statistics.html#data-visualization-solutions",
    "href": "pages/statistics.html#data-visualization-solutions",
    "title": "R Programming Course",
    "section": "Data visualization solutions",
    "text": "Data visualization solutions\n\nContinuous variables\nHere we are going to use the package ggplot2 to visualize a few variables and eventually some relationships between them.\n\n# Visualize the ditribution of weights colored by cardiovascular status\nggplot(data = cv_dis, mapping = aes(x = weight, fill=cardio)) +\n  geom_histogram() +\n  scale_fill_manual(values=c('0'='#6c757d', '1'='#fb8b24'))+\n  facet_grid(cardio~.) +\n  theme_classic()\n\n\n\n\n\n\n\n\nAdditionally, on top of this we can add a density estimation which might be useful to compare our current distribution to a reference distribution like a standard normal one.\n\n# Visualize the ditribution of weights colored by cardiovascular status\nggplot(data = cv_dis, mapping = aes(x = weight, fill=cardio)) +\n  geom_histogram(aes(y=..density..)) + # This is a built-in way to plot a density on an histogram\n  geom_density(color='black', fill=NA) +\n  scale_fill_manual(values=c('0'='#6c757d', '1'='#fb8b24'))+\n  facet_grid(cardio~.) +\n  theme_classic()\n\n\n\n\n\n\n\n\nWe can also plot different relationships between continuous variables in the dataset one versus the other to check for relationships in the data. For example, here we can show the relationship between the different blood pressures. The density normalizes an histogram by taking the single binwidths to the number of observations falling within each bin.\n\n# Visualize the distributions of weights and systolic pressure as a scatterplot colored by cardiovascular status (a bit cluttered)\nggplot(data = cv_dis, aes(x = weight, y = sys_1, col = cardio)) +\n  geom_point(size=3, alpha=0.7) +\n  scale_color_manual(values=c('0'='#6c757d', '1'='#fb8b24'))+\n  theme_classic()\n\n\n\n\n\n\n\n# Visualize the distributions of weights and systolic pressure arranged by cardiovascular status (better)\nggplot(data = cv_dis, aes(x = weight, y = sys_1)) +\n  geom_density2d_filled() +\n  facet_grid(cardio~.) +\n  theme_classic()\n\n\n\n\n\n\n\n\nWhat about distribution of weights across genders using boxplots?\n\n# Gender proportions across cardiac condition\nggplot(cv_dis, aes(x = gender, y=height, fill = gender)) +\n  geom_boxplot(width=0.25)+\n  scale_fill_manual(values=c('F'='#C97B84', 'M'='#388697'))+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nCategorical variables\nIn this case we are going to plot a categorical variable.\n\n# Gender proportions across cardiac condition\nggplot(cv_dis, aes(x = cardio, fill = gender)) +\n  geom_bar(width=0.5)+\n  scale_fill_manual(values=c('F'='#C97B84', 'M'='#388697'))+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nPlot data using GGally\nThe package GGally allows the user to plot mutliple variable pairwise in order to check for relationships within the data.\n\n# Use GGally to plot pairwise relationships with variables\nggpair_plot <- cv_dis %>% select(-one_of('pat_id')) %>%\n  ggpairs()\n\nggpair_plot + theme_classic()"
  },
  {
    "objectID": "pages/statistics.html#hypothesis-testing",
    "href": "pages/statistics.html#hypothesis-testing",
    "title": "R Programming Course",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nHypothesis testing is the process by which we can understand whether variability between two or more distributions is statistically significant. In statistics, we end up having different hypothesis, the null hypothesis (\\(H_0\\)) and the opposite of the null hypothesis (the alternative hypothesis), which is the actual hypothesis (\\(H_1\\)). Successful tests of difference explain the outcome by rejecting the null hypothesis. For instance, a t-test (a parametric test) posits the null hypothesis as the means of two distributions being equal. This means that we are hoping that the difference between the two means is actually 0. Out of all the samples we can pick from a population, the mean of which, we can calculate, the number of outcomes of subtracting the two means will lead to the generation of a probability distribution of the delta-mean being far off 0 given the thruthfulness of the null hypothesis. Since, thanks to the central limit theorem, this distribution will be normal with mean 0, we can set thresholds on the probability distribution over which we can reject the null hypothesis (the actual P-value used). Since the distribution is symmetrical, we can either test for mean differences in both directions (\\(\\mu_a = \\mu_b\\)) with a two-sided test, if we instead already know which is the expected outcome wanted (i.e. that the mean of one distribution should be higher than the other one) we need to use a one-tailed test.\nAs a rule of thumb, non-parametric tests are used when sample sizes are small, data is better represented by the median and/ord we have ordinal data or ranked data.\nSample sizes are able to improve the power of a test since they reduce standard error (\\(\\mu / \\sqrt{n}\\)), which in turn defines the confidence interval around the means of the two samples distributions (which, in the case of a t-test, are defined using a parameter z which comes from the Student’s t distribution). In other cases, we can use parametric tests with small sample sizes in the case we know that the sample we are looking at comes from a normally distributed population (i.e. heights in the population).\nTests that compare different continuous distributions can use specific measures of the distributions (i.e. mean, parametric) or can just be based on the median of the distribution (non-parametric).\nHere we will show a demo of tests and when they are optimally used, first of all we start with testing differences between continuous variables in a parametric way, these can be within group (paired) or across groups (unpaired) and are the t-test (indipendent or paired) and ANOVAs.\nConcerning categorical variables, we can test the significance co-occurrence of two or more by using either a Fisher’s exact test or a \\({\\chi}^2\\) test.\n\n# Pick some data randomly grouped by cardiac condition\nset.seed(111)\ncv_dis_test <- cv_dis %>% \n  mutate_at(vars(gender, chol, gluc, cardio), as.factor) %>% \n  na.omit() %>% \n  group_by(cardio) %>% \n  sample_n(200) %>%  # we can test also paired samples\n  ungroup()\n\n# Two-group comparison, categorical variables (chi-squared or Fisher's exact test)\n# Is cardiovascular disease incidence different between males and females?\n# This test is very powerful with large (>5 in at least a square) sample size, we will use Fisher's instead.\nchisq.test(cv_dis_test$cardio, cv_dis_test$gender)$p.value\n\n[1] 0.1223074\n\n# Do this in tidyverse way\ncv_dis_test %>% \n  summarise(pval_chi = chisq.test(cardio, gender)$p.value,\n            pval_fis = fisher.test(cardio, gender)$p.value)\n\n# A tibble: 1 × 2\n  pval_chi pval_fis\n     <dbl>    <dbl>\n1    0.122    0.122\n\n\nFrom this we can conclude that the test is not significant based on a chi-squared distribution value, therefore we accept the null hypothesis that cardiac conditions are the same across males and females.\nIn this following case, we can instead compare differences between two continuous distributions (systolic pressures across cardiac conditions). Here, given a condition of pseudo-normality, we can use a t-test (independent) to compare the means of the distributions.\n\n## -- Independent t-test\nt.test(sys_1 ~ cardio, data = cv_dis_test)\n\n\n    Welch Two Sample t-test\n\ndata:  sys_1 by cardio\nt = -9.7267, df = 354.9, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -17.28831 -11.47299\nsample estimates:\nmean in group 0 mean in group 1 \n       118.4320        132.8126 \n\nt.test(sys_1 ~ cardio, data = cv_dis_test)$p.value\n\n[1] 5.602789e-20\n\n\nIn this case instead we do see a difference, and in fact, if we plot the distributions and compare them even visually, we can appreciate a visible difference.\n\ncv_dis_test %>% \n  ggplot(aes(x = cardio, y = sys_1)) +\n  geom_boxplot() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe Welch’s correction is telling us that the function is taking care of the difference in variances in the two distributions. This behaviour is default in R. This takes care of the variance assumption (which can be assessed with a Levene test) in a t-test, the other main assumption about normality is actually checked using either a Shapiro-Wilk test or a Kolmogorov-Smirnov test. Both of these tests posit that the null hypothesis is normality, therefore low p-values in these tests mean that the distributions are not normal. With these tests we need to be careful since they can be quite strict, meaning that many times they might indicate to reject the null hypothesis even if the distribution appears quite normal and we feel safe using a parametric test.\n\n# Check for normality\n# graphically check normal distribution\nggplot(cv_dis_test, aes(sys_1)) +\n  geom_histogram(aes(y = stat(density)), binwidth = 15) +\n  stat_function(\n    fun = dnorm, \n    args = list(mean = mean(cv_dis_test$sys_1, na.rm = T), sd = sd(cv_dis_test$sys_1, na.rm = T)), \n    lwd = 2, \n    col = '#fb8b24'\n  ) +\n  facet_wrap( ~ cardio, ncol = 2, scales = \"free_y\") +\n  theme_classic()\n\n\n\n\n\n\n\n# shapiro test\ncardio_0 <- cv_dis_test %>% filter(cardio == 0) %>% pull(sys_1)\ncardio_1 <- cv_dis_test %>% filter(cardio == 1) %>% pull(sys_1)\n\nshapiro.test(cardio_0)\n\n\n    Shapiro-Wilk normality test\n\ndata:  cardio_0\nW = 0.88339, p-value = 2.485e-11\n\nshapiro.test(cardio_1)\n\n\n    Shapiro-Wilk normality test\n\ndata:  cardio_1\nW = 0.95269, p-value = 3.457e-06\n\ncv_dis_test %>% \n  group_by(cardio) %>% \n  summarise(shap_test = shapiro.test(sys_1)$p.value)\n\n# A tibble: 2 × 2\n  cardio shap_test\n  <fct>      <dbl>\n1 0       2.48e-11\n2 1       3.46e- 6\n\n\nIn the case we need to use a non-parametric test, we can use a Mann-Withney U test or a Wilcoxon-ranked-sum test. The first is for paired data while the second is for unpaired data. In R, these two are both implemented in a single function as shown below.\n\ncv_dis_test %>% \n  summarise(pval_t      = wilcox.test(sys_1 ~ cardio)$p.value,\n            pval_mw_pair = wilcox.test(sys_1 ~ cardio, paired = T)$p.value)\n\n# A tibble: 1 × 2\n    pval_t pval_mw_pair\n     <dbl>        <dbl>\n1 1.67e-18     1.55e-16\n\n\nIn the scenario of comparing multiple distributions, using ANOVA we need to keep in mind that it is just referring p-values for differences in means from one to the other samples. In this way, we will then need a post-hoc test in order to check pairwise differences between means."
  },
  {
    "objectID": "pages/prog_base.html",
    "href": "pages/prog_base.html",
    "title": "R Programming Course",
    "section": "",
    "text": "The existence of programming languages is strongly related to how computers work and how they have been developed during their history. Architecture is the base on which languages are built. Computers are built and organized in what is known as Von Neumann architecture, in this sense we have a CPU which is the main component involved in calculations. This is in turn composed of an arithmetic/logic unit a control unit and registers (MAR and MDR) on which data is kept pre-operation after it has been sourced from the memory unit (RAM). In between the memory and the CPU lies the cache (L1, L2) which hosts information that can be retrieved in a faster way than from memory.\n\n\n\n\n\n\nSo, what is a program? Memory and CPUs can only understand numbers, specifically bits of information, 0s and 1s and are only able to perform very simple operations, meanwhile higher-level operations are achieved by using algorithms which compound simpler operations iteratively. The first step towards translating bit instructions into alphanumeric meaningful words was the Assembly language. Assembly worked by using procedure (or routines) calling, we can think of routines as blocks of codes to execute in larger scripts. Part of this process was also involving memory management procedures, with pre-allocated memory (static) to store all global variables in a last-in first-out way and this set the initial mechanism for running programs, other parts of memory include the stack, which is the part related to local variables and programs and then a part of the memory reserved to code (or text in general), called text and finally a part known as heap which is, in line with its name, very dynamic and changes in relation to changes in need at runtime, this part of memory is the one which can be controlled by the programmer (in compiled languages usually) or by programs themselves (in interpreted languages usually) as well, here programs can be restricted in terms of usage. This was mainly engineered to avoid memory conflicts between running programs, so it became crucial to control the system and assign memory to running processes in a consistent and stable manner. This task was tackled by developing operating systems (OS).\n\n\n\nThe OS is instrumental in establishing order in resource and memory management across programs running on the same machine from the same or different users. Another step towards this is to share a machine’s CPU for time-sharing across processes running at the same time, all this is programmed within the OS, which is tasked with managing resources. In the 80s and 90s, with the advent of networking, distributed systems were created in which programs could be run on different machines by taking advantage of web-based protocols (like ssh). After the initial rise of OSs, forms of mass storage were needed in order to have a memory support built into the computer itself, this started with CDs and then hard-drives, which nowadays have been substituted in large by faster technologies such as SSDs and cloud-based solutions for storing data.\n\n\n\nAll along, expressive languages were also developed which were needed to get a more abstract sense of operations and allow for more comprehensible syntax for more complex operations like cycling and conditional execution, introduced a grammar based on data structures, types and objects. Ultimately this allows the programmer to focus on the actual program itself more than its implementation.\nSo summed up, the advantages of languages include: + Loops + Conditional execution + Code blocks + Operators + Data structures (specifications) + Function definition\nObjects in memory are always stored sequentially and specifically, matrices and tabular data in Fortran systems (and the majority of other languages) are stored column-wise, so trying to index a matrix like A[57] == A[7,6] means that the 57th element of a matrix in memory is equal to the 7th element on the 6th column in the R object (which is 1-indexed).\n\nindex <- c(1,2,3,NA)\nrandom <- runif(100)\n\n# Try to use NA as index (should return an NA in that index position)\n# This highlights a fault in R's permessibility, here it does not alert the use for the sake of giving on answer\nrandom[index]\n\n[1] 0.7537029 0.3882457 0.2533682        NA\n\n\n\n\n\nIn the above case we can check that R, an interpreted language, masks the user with the ability to manage an eventual error for the sake of code usability, the user then misses a chance to catch an eventual mistake in the code, maybe while generating the index above, an NA was not wanted, but R does not signal its presence while subsetting random. This mechanisms of silent coercion are widespread in R and the user should always be aware of when they can happen since they can represent a scenarion of bug generation and data loss."
  },
  {
    "objectID": "pages/prog_base.html#parallel-programming",
    "href": "pages/prog_base.html#parallel-programming",
    "title": "R Programming Course",
    "section": "Parallel Programming",
    "text": "Parallel Programming\n\nThe Basics\nWhen programmming something using interpreted (vs compiled, i.e. C++) languages like R, primarily thought as interactive, it is important to stop and understand how functions work in terms of operations and I/O instead of just getting into the thick of it and start using up resources. This is primarily concerning memory requirements when working on a shared platform as well as actual CPU resources. Dynamic vs Static libraries. To provide an example of the performance that can be obtained by programming in compiled languages, let’s imagine a scenario by which we have a series of DNA sequences and we want to get all of the 3-base kmers which are present in each sequence. One relatively straightforward way would be to first generate an empty sequence by kmer matrix and then, after having determined all the possible kmers in the sequences (a dictionary), start counting and indexing any occurrence into the appropriate position. Although this works well with short sequences, the advantages which come from the conjunction of binary encoding and the use of C++ which allows the direct accession of space on memory grant amazing advantages. Brush-up on the definition of a core and a thread? But another way to brute-force achieving speed is to use more cores and threads! In order to take advantage of the presence of multiple cores, we can run different processes by sizing the data into smaller chunks (easier, less efficient) or make a program multithread, this is the best way possible but also the most expensive in terms of implementation time. By multithreading we split up the instructions of a program into spawning different children processes. Parallelization comes with a few challenges, including timing and managing execution I/O across child processes. There are cases in which data needs to be modified by different processes thus we need to ensure that we are not compromising the outputs and/or mixing information from different processes.\n\n\nGraphics Processing Units\nGPUs work very well for linear algebra calculations, especially linear algebra concepts and operations. GPUs are very “dumb” compare to CPUs in terms of the kinds of operations and the complexity of operations that they can do, but what they were initially designed to do (rendering graphics, therefore large matrices) proved also very powerful when applied to operations involving matrices. These cards have a very different architecture as opposed to the Von Neumann one seen previosuly, they contain many more threads doing the same operation on the same data, which needs, crucially, to be copied on the CPU itself, which will also store the final object after computation which will need to be re-imported into memory.\n\n\nOOP History and Paradigms\nProgramming has taken up many shapes during its history, we can have unstructured programming which then evolved into procedural programming by definig re-usable functions which otherwise need repetition. The next step in complexity is represented by modular programming, by which modules of code are utilized and interconnect to provide functionality across different files. The final step is represented by the object-oriented paradigm, by which objects are identified by a set of methods coupled with the needed data, which in turn is strictly related to the methods. Let’s imagine that we want to create a new genomics object in which to store genomic data, this object can be as general as possible, with specific properties and methods. The former might contain IDs from different classification systems, the annotation and the organism of origin while the latter might instead contain actions which can be performed on the object such as alignment, changes in bases or calculation of GC content etc… In contrast to procedural programming, the program flow in OOP is said to be “message oriented”, by this meaning that the user talks to objects by sending messages through methods. The interesting part of OOP is that classes are not just created to be on their own, instead relationships between objects is desirable as a distinguished feature, for example an object representing a transcript can be part of an object representing a chromosome and can inherit actions which can be performed on other elements from the chromosome object, for example determining start and end locations, which could also be use for a genomic sequence, which in turn can be another class under the chromosome one. Methods defined in a parent class are re-implemented in the daughter classes, in this way in the code we can always call the same method and the correct implementation will always be selected since it is specified in the appropriate class, this is called polymorphism."
  }
]